{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3daf14fe",
   "metadata": {},
   "source": [
    "### NYSE + NASDAQ progressions\n",
    "- take in txt with meta data \n",
    "- find Symbols that make up 80% of Total market Cap \n",
    "- ensure symbols are formatted for YFinance API \n",
    "- get daily OHLCV data from Yfinance for each, save in SQL db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bb9141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from NYSE.csv\n",
      "Found 2358 unique symbols with a total market cap of $41,402,108,200,657.00\n",
      "Keeping 126 symbols (5.34% of total) that represent 60.14% of total market cap\n",
      "Selected tickers: ['BRK/B', 'BRK/A', 'TSM', 'LLY', 'WMT']...\n",
      "Converted 126 tickers to yfinance format\n",
      "Ready to use with yfinance!\n",
      "Successfully loaded data from NASDAQ.csv\n",
      "Found 3863 unique symbols with a total market cap of $31,488,945,564,668.00\n",
      "Keeping 17 symbols (0.44% of total) that represent 60.62% of total market cap\n",
      "Selected tickers: ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'GOOG']...\n",
      "Converted 17 tickers to yfinance format\n",
      "Ready to use with yfinance!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_stock_data(file_path):\n",
    "    \"\"\"Load stock data from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded data from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def filter_columns(df):\n",
    "    \"\"\"Keep only Symbol and Market Cap columns.\"\"\"\n",
    "    if \"Symbol\" in df.columns and \"Market Cap\" in df.columns:\n",
    "        filtered_df = df[[\"Symbol\", \"Market Cap\"]].copy()\n",
    "        # Remove any rows with NaN values\n",
    "        filtered_df = filtered_df.dropna()\n",
    "        \n",
    "        # Ensure Market Cap is numeric\n",
    "        filtered_df[\"Market Cap\"] = pd.to_numeric(filtered_df[\"Market Cap\"], errors=\"coerce\")\n",
    "        filtered_df = filtered_df.dropna()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_symbols = filtered_df.shape[0]\n",
    "        total_market_cap = filtered_df[\"Market Cap\"].sum()\n",
    "        \n",
    "        print(f\"Found {total_symbols} unique symbols with a total market cap of ${total_market_cap:,.2f}\")\n",
    "        return filtered_df\n",
    "    else:\n",
    "        print(\"Error: Required columns 'Symbol' and 'Market Cap' not found in the data\")\n",
    "        return None\n",
    "\n",
    "def apply_pareto_principle(df, top_percent_market_cap=80):\n",
    "    \"\"\"\n",
    "    Find the top X symbols that make up Y% of the total market cap.\n",
    "    Returns the filtered DataFrame and the percentage of symbols this represents.\n",
    "    \"\"\"\n",
    "    # Sort by Market Cap in descending order\n",
    "    sorted_df = df.sort_values(\"Market Cap\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate cumulative percentage of market cap\n",
    "    total_market_cap = sorted_df[\"Market Cap\"].sum()\n",
    "    sorted_df[\"Cumulative Market Cap\"] = sorted_df[\"Market Cap\"].cumsum()\n",
    "    sorted_df[\"Cumulative Percentage\"] = (sorted_df[\"Cumulative Market Cap\"] / total_market_cap) * 100\n",
    "    \n",
    "    # Find where we hit the target percentage\n",
    "    target_df = sorted_df[sorted_df[\"Cumulative Percentage\"] <= top_percent_market_cap]\n",
    "    \n",
    "    # If empty (happens if first row already exceeds the percentage), take at least one row\n",
    "    if target_df.empty:\n",
    "        target_df = sorted_df.iloc[:1]\n",
    "    else:\n",
    "        # Add one more row to ensure we're at or above the target percentage\n",
    "        next_index = target_df.index[-1] + 1\n",
    "        if next_index < len(sorted_df):\n",
    "            target_df = pd.concat([target_df, sorted_df.iloc[[next_index]]])\n",
    "    \n",
    "    symbols_percent = (len(target_df) / len(sorted_df)) * 100\n",
    "    actual_market_cap_percent = target_df[\"Market Cap\"].sum() / total_market_cap * 100\n",
    "    \n",
    "    print(f\"Keeping {len(target_df)} symbols ({symbols_percent:.2f}% of total) that represent {actual_market_cap_percent:.2f}% of total market cap\")\n",
    "    \n",
    "    return target_df\n",
    "\n",
    "def get_ticker_list(df):\n",
    "    \"\"\"Extract ticker symbols from DataFrame.\"\"\"\n",
    "    return df[\"Symbol\"].tolist()\n",
    "\n",
    "def convert_tickers_for_yfinance(tickers):\n",
    "    \"\"\"Convert ticker symbols to yfinance format.\"\"\"\n",
    "    converted_tickers = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Convert to string in case it's not already\n",
    "        ticker_str = str(ticker)\n",
    "        # Replace '^', '/', and '.' with '-'\n",
    "        converted = re.sub(r'[\\^/\\.]', '-', ticker_str)\n",
    "        converted_tickers.append(converted)\n",
    "    \n",
    "    print(f\"Converted {len(converted_tickers)} tickers to yfinance format\")\n",
    "    return converted_tickers\n",
    "\n",
    "def clean_tickers(file_path, pareto_percent=99):\n",
    "    # File path - you'll need to update this\n",
    "    file_path = file_path \n",
    "    \n",
    "    # Step 1: Load the CSV file\n",
    "    df = load_stock_data(file_path)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Step 2: Filter columns\n",
    "    filtered_df = filter_columns(df)\n",
    "    if filtered_df is None:\n",
    "        return\n",
    "    \n",
    "    # Step 3: Apply Pareto principle (adjust the percentage as needed)\n",
    "    target_market_cap_percent = pareto_percent  # Target percentage of market cap to capture\n",
    "    pareto_df = apply_pareto_principle(filtered_df, target_market_cap_percent)\n",
    "    \n",
    "    # Step 4: Get list of tickers\n",
    "    tickers = get_ticker_list(pareto_df)\n",
    "    print(f\"Selected tickers: {tickers[:5]}{'...' if len(tickers) > 5 else ''}\")\n",
    "    \n",
    "    # Step 5: Convert tickers for yfinance\n",
    "    yfinance_tickers = convert_tickers_for_yfinance(tickers)\n",
    "    \n",
    "    # You can now use these tickers with yfinance\n",
    "    print(\"Ready to use with yfinance!\")\n",
    "    \n",
    "    return yfinance_tickers\n",
    "\n",
    "# Get tickers for both exchanges using the clean_tickers function\n",
    "NYSE_tickers = clean_tickers(file_path=\"NYSE.csv\", pareto_percent=60)\n",
    "NASDAQ_tickers = clean_tickers(file_path=\"NASDAQ.csv\", pareto_percent=60) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93f80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:51:11,169 - INFO - Combined 126 NYSE tickers and 17 NASDAQ tickers into 143 unique tickers\n",
      "2025-04-21 20:51:11,170 - INFO - Database created/connected successfully at stock_market_data.db\n",
      "2025-04-21 20:51:11,172 - INFO - Starting download of 143 tickers from 2024-01-01 to 2025-04-21\n",
      "2025-04-21 20:51:11,172 - INFO - Processing chunk 1/1 (143 tickers)\n",
      "2025-04-21 20:51:15,228 - INFO - Download complete. Successful: 143, Failed: 0\n",
      "2025-04-21 20:51:15,478 - INFO - Database Statistics:\n",
      "2025-04-21 20:51:15,478 - INFO - Total records: 2392952\n",
      "2025-04-21 20:51:15,479 - INFO - Unique symbols: 1552\n",
      "2025-04-21 20:51:15,479 - INFO - Date range: 2019-01-02 to 2025-04-21\n",
      "2025-04-21 20:51:15,479 - INFO - Status counts: [('SUCCESS', 1552)]\n",
      "2025-04-21 20:51:15,480 - INFO - Data download complete. Database saved to stock_market_data.db\n",
      "2025-04-21 20:51:15,480 - INFO - Downloaded data for 143 tickers successfully. Failed for 0 tickers.\n",
      "2025-04-21 20:51:15,481 - INFO - Database connection closed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"stock_data_download.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def create_database(db_path=\"stock_data.db\"):\n",
    "    \"\"\"Create SQLite database with tables for stock data.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table for daily stock data\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS stock_daily_data (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            symbol TEXT NOT NULL,\n",
    "            date TEXT NOT NULL,\n",
    "            open REAL,\n",
    "            high REAL,\n",
    "            low REAL,\n",
    "            close REAL,\n",
    "            volume INTEGER,\n",
    "            UNIQUE(symbol, date)\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Create index for faster queries\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol_date ON stock_daily_data (symbol, date)')\n",
    "        \n",
    "        # Create table for ticker metadata\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS ticker_metadata (\n",
    "            symbol TEXT PRIMARY KEY,\n",
    "            source TEXT,\n",
    "            last_updated TEXT,\n",
    "            status TEXT\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        logger.info(f\"Database created/connected successfully at {db_path}\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating database: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_stock_data(tickers, conn, start_date='2020-01-01', end_date=None, \n",
    "                        interval='1d', source='combined', chunk_size=50):\n",
    "    \"\"\"\n",
    "    Download stock data for multiple tickers and save to SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        tickers: List of ticker symbols\n",
    "        conn: SQLite connection\n",
    "        start_date: Start date for data download\n",
    "        end_date: End date for data download (defaults to today)\n",
    "        interval: Data interval ('1d', '1wk', etc.)\n",
    "        source: Source of tickers for metadata\n",
    "        chunk_size: Number of tickers to download at once\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Parse dates\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    today = pd.to_datetime(datetime.now().date())\n",
    "    \n",
    "    if end_dt > today:\n",
    "        logger.info(f\"End date {end_dt.date()} is in the future; adjusting to today's date {today.date()}.\")\n",
    "        end_dt = today\n",
    "    \n",
    "    # Process tickers in chunks to avoid API limits\n",
    "    total_tickers = len(tickers)\n",
    "    successful_downloads = 0\n",
    "    failed_downloads = 0\n",
    "    \n",
    "    logger.info(f\"Starting download of {total_tickers} tickers from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Process in chunks\n",
    "    for i in range(0, total_tickers, chunk_size):\n",
    "        chunk = tickers[i:i+chunk_size]\n",
    "        logger.info(f\"Processing chunk {i//chunk_size + 1}/{(total_tickers-1)//chunk_size + 1} ({len(chunk)} tickers)\")\n",
    "        \n",
    "        try:\n",
    "            # Download data for chunk\n",
    "            df = yf.download(\n",
    "                tickers=chunk,\n",
    "                start=start_dt.strftime('%Y-%m-%d'),\n",
    "                end=(end_dt + timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "                interval=interval,\n",
    "                group_by='ticker',\n",
    "                auto_adjust=True,\n",
    "                prepost=False,\n",
    "                threads=True,\n",
    "                progress=False\n",
    "            )\n",
    "            \n",
    "            # Check if we got data\n",
    "            if df.empty:\n",
    "                logger.warning(f\"No data returned for chunk {i//chunk_size + 1}\")\n",
    "                failed_downloads += len(chunk)\n",
    "                continue\n",
    "                \n",
    "            # Process each ticker in the chunk\n",
    "            for ticker in chunk:\n",
    "                try:\n",
    "                    # For a single ticker, the dataframe is not multi-indexed\n",
    "                    if len(chunk) == 1:\n",
    "                        ticker_df = df.copy()\n",
    "                    else:\n",
    "                        # For multiple tickers, extract the specific ticker data\n",
    "                        ticker_df = df[ticker].copy()\n",
    "                    \n",
    "                    # Skip if no data for this ticker\n",
    "                    if ticker_df.empty:\n",
    "                        logger.warning(f\"No data for {ticker}\")\n",
    "                        failed_downloads += 1\n",
    "                        \n",
    "                        # Update metadata table with failed status\n",
    "                        cursor = conn.cursor()\n",
    "                        cursor.execute('''\n",
    "                        INSERT OR REPLACE INTO ticker_metadata (symbol, source, last_updated, status)\n",
    "                        VALUES (?, ?, ?, ?)\n",
    "                        ''', (ticker, source, datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'FAILED'))\n",
    "                        conn.commit()\n",
    "                        continue\n",
    "                    \n",
    "                    # Reset index to make date a column\n",
    "                    ticker_df = ticker_df.reset_index()\n",
    "                    \n",
    "                    # Ensure we have the expected columns\n",
    "                    required_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "                    \n",
    "                    # Check and rename columns if needed\n",
    "                    if 'Datetime' in ticker_df.columns:\n",
    "                        ticker_df = ticker_df.rename(columns={'Datetime': 'Date'})\n",
    "                    \n",
    "                    # Ensure all required columns exist\n",
    "                    missing_columns = [col for col in required_columns if col not in ticker_df.columns]\n",
    "                    if missing_columns:\n",
    "                        logger.warning(f\"Missing columns for {ticker}: {missing_columns}\")\n",
    "                        failed_downloads += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert date to string format for SQLite\n",
    "                    ticker_df['Date'] = ticker_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "                    \n",
    "                    # Insert data into database\n",
    "                    cursor = conn.cursor()\n",
    "                    \n",
    "                    # Prepare data for insertion\n",
    "                    data_to_insert = []\n",
    "                    for _, row in ticker_df.iterrows():\n",
    "                        data_to_insert.append((\n",
    "                            ticker,\n",
    "                            row['Date'],\n",
    "                            row['Open'],\n",
    "                            row['High'],\n",
    "                            row['Low'],\n",
    "                            row['Close'],\n",
    "                            row['Volume']\n",
    "                        ))\n",
    "                    \n",
    "                    # Use executemany for better performance\n",
    "                    cursor.executemany('''\n",
    "                    INSERT OR REPLACE INTO stock_daily_data \n",
    "                    (symbol, date, open, high, low, close, volume)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', data_to_insert)\n",
    "                    \n",
    "                    # Update metadata\n",
    "                    cursor.execute('''\n",
    "                    INSERT OR REPLACE INTO ticker_metadata (symbol, source, last_updated, status)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                    ''', (ticker, source, datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'SUCCESS'))\n",
    "                    \n",
    "                    conn.commit()\n",
    "                    successful_downloads += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {ticker}: {e}\")\n",
    "                    failed_downloads += 1\n",
    "                    \n",
    "                    # Update metadata with error status\n",
    "                    try:\n",
    "                        cursor = conn.cursor()\n",
    "                        cursor.execute('''\n",
    "                        INSERT OR REPLACE INTO ticker_metadata (symbol, source, last_updated, status)\n",
    "                        VALUES (?, ?, ?, ?)\n",
    "                        ''', (ticker, source, datetime.now().strftime('%Y-%m-%d %H:%M:%S'), f'ERROR: {str(e)[:100]}'))\n",
    "                        conn.commit()\n",
    "                    except Exception as inner_e:\n",
    "                        logger.error(f\"Error updating metadata for {ticker}: {inner_e}\")\n",
    "            \n",
    "            # Sleep between chunks to avoid rate limiting\n",
    "            if i + chunk_size < total_tickers:\n",
    "                logger.info(f\"Sleeping for 2 seconds between chunks...\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading chunk {i//chunk_size + 1}: {e}\")\n",
    "            failed_downloads += len(chunk)\n",
    "    \n",
    "    logger.info(f\"Download complete. Successful: {successful_downloads}, Failed: {failed_downloads}\")\n",
    "    return successful_downloads, failed_downloads\n",
    "\n",
    "def get_database_stats(conn):\n",
    "    \"\"\"Get statistics about the database.\"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get total number of records\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM stock_daily_data\")\n",
    "        total_records = cursor.fetchone()[0]\n",
    "        \n",
    "        # Get number of unique symbols\n",
    "        cursor.execute(\"SELECT COUNT(DISTINCT symbol) FROM stock_daily_data\")\n",
    "        unique_symbols = cursor.fetchone()[0]\n",
    "        \n",
    "        # Get date range\n",
    "        cursor.execute(\"SELECT MIN(date), MAX(date) FROM stock_daily_data\")\n",
    "        date_range = cursor.fetchone()\n",
    "        \n",
    "        # Get success/failure counts\n",
    "        cursor.execute(\"SELECT status, COUNT(*) FROM ticker_metadata GROUP BY status\")\n",
    "        status_counts = cursor.fetchall()\n",
    "        \n",
    "        logger.info(f\"Database Statistics:\")\n",
    "        logger.info(f\"Total records: {total_records}\")\n",
    "        logger.info(f\"Unique symbols: {unique_symbols}\")\n",
    "        logger.info(f\"Date range: {date_range[0]} to {date_range[1]}\")\n",
    "        logger.info(f\"Status counts: {status_counts}\")\n",
    "        \n",
    "        return {\n",
    "            \"total_records\": total_records,\n",
    "            \"unique_symbols\": unique_symbols,\n",
    "            \"date_range\": date_range,\n",
    "            \"status_counts\": status_counts\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting database stats: {e}\")\n",
    "        return None\n",
    "\n",
    "def GetData(start_date, end_date):\n",
    "    # Combine ticker lists from your previous code\n",
    "    # Assuming NYSE_tickers and NASDAQ_tickers are already defined from your previous code\n",
    "    combined_tickers = list(set(NYSE_tickers + NASDAQ_tickers))  # Remove duplicates\n",
    "    logger.info(f\"Combined {len(NYSE_tickers)} NYSE tickers and {len(NASDAQ_tickers)} NASDAQ tickers into {len(combined_tickers)} unique tickers\")\n",
    "    \n",
    "    # Create database connection\n",
    "    db_path = \"stock_market_data.db\"\n",
    "    conn = create_database(db_path)\n",
    "    \n",
    "    if conn is None:\n",
    "        logger.error(\"Failed to create database connection. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Set date range - adjust as needed\n",
    "        start_date = start_date\n",
    "        end_date = end_date\n",
    "\n",
    "        # Download data\n",
    "        successful, failed = download_stock_data(\n",
    "            tickers=combined_tickers,\n",
    "            conn=conn,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            interval='1d',\n",
    "            source='combined',\n",
    "            chunk_size=500  # Adjust based on API limits\n",
    "        )\n",
    "        \n",
    "        # Get database statistics\n",
    "        stats = get_database_stats(conn)\n",
    "        \n",
    "        logger.info(f\"Data download complete. Database saved to {db_path}\")\n",
    "        logger.info(f\"Downloaded data for {successful} tickers successfully. Failed for {failed} tickers.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main process: {e}\")\n",
    "    finally:\n",
    "        # Close connection\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            logger.info(\"Database connection closed\")\n",
    "\n",
    "#Update Database \n",
    "start_date = '2024-01-01'  # 5 years of data\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')  # Today\n",
    "GetData(start_date=start_date, end_date=end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:51:15,734 - INFO - Calculating A/D data for all tickers in the database\n",
      "2025-04-21 20:51:15,867 - INFO - Using date range from database: 2019-01-02 to 2025-04-21\n",
      "2025-04-21 20:51:16,576 - INFO - Processing 1552 tickers\n",
      "/var/folders/0l/68dwwf7n4k14jjgryv9tt5mw0000gn/T/ipykernel_71144/4168156369.py:97: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['return'] = df['close'].pct_change()\n",
      "/var/folders/0l/68dwwf7n4k14jjgryv9tt5mw0000gn/T/ipykernel_71144/4168156369.py:97: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['return'] = df['close'].pct_change()\n",
      "/var/folders/0l/68dwwf7n4k14jjgryv9tt5mw0000gn/T/ipykernel_71144/4168156369.py:97: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['return'] = df['close'].pct_change()\n",
      "/var/folders/0l/68dwwf7n4k14jjgryv9tt5mw0000gn/T/ipykernel_71144/4168156369.py:97: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df['return'] = df['close'].pct_change()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Set up logging if not already done\n",
    "if not logging.getLogger().hasHandlers():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"ad_analysis.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def get_database_date_range(db_path=\"stock_market_data.db\"):\n",
    "    \"\"\"\n",
    "    Get the date range available in the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get the min and max dates from the database\n",
    "        cursor.execute(\"SELECT MIN(date), MAX(date) FROM stock_daily_data\")\n",
    "        min_date, max_date = cursor.fetchone()\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return pd.to_datetime(min_date), pd.to_datetime(max_date)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting database date range: {e}\")\n",
    "        # Fallback to a reasonable default\n",
    "        today = datetime.now()\n",
    "        return today - timedelta(days=365*2), today\n",
    "\n",
    "def calculate_ad_data_from_db(db_path=\"stock_market_data.db\"):\n",
    "    \"\"\"\n",
    "    Calculate Advance-Decline data using all tickers in our SQL database\n",
    "    \n",
    "    Parameters:\n",
    "    db_path (str): Path to the SQLite database\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with A/D data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Calculating A/D data for all tickers in the database\")\n",
    "    \n",
    "    # Get date range from database\n",
    "    start_date, end_date = get_database_date_range(db_path)\n",
    "    logger.info(f\"Using date range from database: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Get all unique dates in the database\n",
    "        date_query = \"SELECT DISTINCT date FROM stock_daily_data ORDER BY date\"\n",
    "        all_dates = pd.read_sql_query(date_query, conn)['date']\n",
    "        \n",
    "        # Initialize DataFrame for A/D data\n",
    "        ad_data = pd.DataFrame(index=all_dates)\n",
    "        ad_data.index = pd.to_datetime(ad_data.index)\n",
    "        \n",
    "        # Initialize columns\n",
    "        ad_data['Advances'] = 0\n",
    "        ad_data['Declines'] = 0\n",
    "        ad_data['Unchanged'] = 0\n",
    "        \n",
    "        # Get list of all tickers in the database\n",
    "        ticker_query = \"SELECT DISTINCT symbol FROM stock_daily_data\"\n",
    "        all_tickers = pd.read_sql_query(ticker_query, conn)['symbol'].tolist()\n",
    "        \n",
    "        logger.info(f\"Processing {len(all_tickers)} tickers\")\n",
    "        \n",
    "        # Process each ticker\n",
    "        for ticker in all_tickers:\n",
    "            # Query the database for this ticker's data\n",
    "            query = f\"\"\"\n",
    "            SELECT date, close \n",
    "            FROM stock_daily_data \n",
    "            WHERE symbol = '{ticker}' \n",
    "            ORDER BY date\n",
    "            \"\"\"\n",
    "            \n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            \n",
    "            # Skip if no data or only one data point\n",
    "            if len(df) <= 1:\n",
    "                continue\n",
    "                \n",
    "            # Calculate returns\n",
    "            df['return'] = df['close'].pct_change()\n",
    "            \n",
    "            # Skip the first row (NaN return)\n",
    "            df = df.iloc[1:]\n",
    "            \n",
    "            # Classify each day's return\n",
    "            for _, row in df.iterrows():\n",
    "                date = row['date']\n",
    "                ret = row['return']\n",
    "                \n",
    "                if pd.to_datetime(date) in ad_data.index:\n",
    "                    if ret > 0:\n",
    "                        ad_data.loc[pd.to_datetime(date), 'Advances'] += 1\n",
    "                    elif ret < 0:\n",
    "                        ad_data.loc[pd.to_datetime(date), 'Declines'] += 1\n",
    "                    else:\n",
    "                        ad_data.loc[pd.to_datetime(date), 'Unchanged'] += 1\n",
    "        \n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "        \n",
    "        # Calculate Daily A/D and Cumulative A/D\n",
    "        ad_data['Daily_AD'] = ad_data['Advances'] - ad_data['Declines']\n",
    "        ad_data['Cumulative_AD'] = ad_data['Daily_AD'].cumsum()\n",
    "        \n",
    "        logger.info(f\"Successfully calculated A/D data\")\n",
    "        return ad_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating A/D data: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def get_spy_data_from_db(db_path=\"stock_market_data.db\"):\n",
    "    \"\"\"\n",
    "    Get SPY data from the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get date range from database\n",
    "        start_date, end_date = get_database_date_range(db_path)\n",
    "        \n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Query the database for SPY data\n",
    "        query = f\"\"\"\n",
    "        SELECT date, close \n",
    "        FROM stock_daily_data \n",
    "        WHERE symbol = 'SPY' \n",
    "        ORDER BY date\n",
    "        \"\"\"\n",
    "        \n",
    "        spy = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        # Convert date to datetime\n",
    "        spy['date'] = pd.to_datetime(spy['date'])\n",
    "        spy = spy.set_index('date')\n",
    "        \n",
    "        if len(spy) == 0:\n",
    "            raise ValueError(\"SPY data not found in database\")\n",
    "        \n",
    "        return spy\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting SPY data from database: {e}\")\n",
    "        # If we can't get SPY from the database, fall back to yfinance\n",
    "        logger.info(\"Falling back to yfinance for SPY data\")\n",
    "        import yfinance as yf\n",
    "        \n",
    "        # Use the database date range if available\n",
    "        try:\n",
    "            start_date, end_date = get_database_date_range(db_path)\n",
    "            spy = yf.download('SPY', start=start_date, end=end_date + timedelta(days=1))\n",
    "        except:\n",
    "            # If all else fails, get 2 years of data\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=365*2)\n",
    "            spy = yf.download('SPY', start=start_date, end=end_date)\n",
    "        \n",
    "        return spy[['Close']].rename(columns={'Close': 'close'})\n",
    "\n",
    "def plot_ad_analysis(db_path=\"stock_market_data.db\"):\n",
    "    \"\"\"\n",
    "    Calculate and plot A/D analysis for all tickers in the database\n",
    "    \"\"\"\n",
    "    # Calculate A/D data\n",
    "    ad_data = calculate_ad_data_from_db(db_path)\n",
    "    \n",
    "    # Check if we have data\n",
    "    if len(ad_data) == 0:\n",
    "        logger.error(\"Failed to calculate A/D data\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate 90-day MA of Cumulative A/D\n",
    "    ad_data['Cumulative_AD_90MA'] = ad_data['Cumulative_AD'].rolling(window=30).mean()\n",
    "    \n",
    "    # Calculate red background indicator\n",
    "    ad_data['Red_Background'] = (ad_data['Cumulative_AD'] < ad_data['Cumulative_AD_90MA']).astype(int)\n",
    "    \n",
    "    # Get SPY data\n",
    "    spy = get_spy_data_from_db(db_path)\n",
    "    \n",
    "    # Align SPY data with our A/D data\n",
    "    common_dates = ad_data.index.intersection(spy.index)\n",
    "    ad_data = ad_data.loc[common_dates]\n",
    "    spy = spy.loc[common_dates]\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: SPY price\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(spy.index, spy['close'], 'k-')\n",
    "    plt.title('SPY Price')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: A/D Line with 90-day MA\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(ad_data.index, ad_data['Cumulative_AD'], 'b-', label='Cumulative A/D')\n",
    "    plt.plot(ad_data.index, ad_data['Cumulative_AD_90MA'], 'r-', label='90-day MA')\n",
    "    plt.title('Market-Wide Advance-Decline Line')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: SPY with Red Background\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(spy.index, spy['close'], 'k-')\n",
    "    \n",
    "    # Highlight red background periods - fixed version\n",
    "    red_periods = []\n",
    "    current_start = None\n",
    "    \n",
    "    for i in range(len(ad_data)):\n",
    "        is_red = ad_data['Red_Background'].iloc[i] == 1\n",
    "        \n",
    "        if is_red and current_start is None:\n",
    "            current_start = ad_data.index[i]\n",
    "        elif not is_red and current_start is not None:\n",
    "            red_periods.append((current_start, ad_data.index[i]))\n",
    "            current_start = None\n",
    "    \n",
    "    # Handle case where we end on a red period\n",
    "    if current_start is not None:\n",
    "        red_periods.append((current_start, ad_data.index[-1]))\n",
    "    \n",
    "    # Apply the red background for each period\n",
    "    for start, end in red_periods:\n",
    "        plt.axvspan(start, end, alpha=0.2, color='red')\n",
    "    \n",
    "    plt.title('SPY with Red Background Indicator')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the data for further analysis if needed\n",
    "    return {\n",
    "        'ad_data': ad_data,\n",
    "        'spy': spy\n",
    "    }\n",
    "\n",
    "def calculate_market_statistics(results):\n",
    "    \"\"\"\n",
    "    Calculate additional market statistics from the A/D analysis\n",
    "    \"\"\"\n",
    "    if results is None or 'ad_data' not in results or 'spy' not in results:\n",
    "        logger.error(\"Invalid results for market statistics calculation\")\n",
    "        return None\n",
    "    \n",
    "    ad_data = results['ad_data']\n",
    "    spy = results['spy']\n",
    "    \n",
    "    # Make sure the indexes are aligned\n",
    "    common_dates = ad_data.index.intersection(spy.index)\n",
    "    ad_aligned = ad_data.loc[common_dates]\n",
    "    spy_aligned = spy.loc[common_dates]\n",
    "    \n",
    "    # Calculate correlation between SPY and A/D line\n",
    "    correlation = spy_aligned['close'].corr(ad_aligned['Cumulative_AD'])\n",
    "    \n",
    "    # Calculate percentage of days with red background\n",
    "    red_days_pct = ad_aligned['Red_Background'].mean() * 100\n",
    "    \n",
    "    # Calculate SPY returns\n",
    "    spy_returns = spy_aligned['close'].pct_change().dropna()\n",
    "    \n",
    "    # Get corresponding red background values\n",
    "    red_background = ad_aligned['Red_Background'].loc[spy_returns.index]\n",
    "    \n",
    "    # Separate returns for red and non-red periods\n",
    "    red_returns = spy_returns[red_background == 1]\n",
    "    non_red_returns = spy_returns[red_background == 0]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_red_return = red_returns.mean() * 100 if len(red_returns) > 0 else 0\n",
    "    avg_non_red_return = non_red_returns.mean() * 100 if len(non_red_returns) > 0 else 0\n",
    "    \n",
    "    # Calculate cumulative return during red and non-red periods\n",
    "    cum_red_return = ((1 + red_returns).prod() - 1) * 100 if len(red_returns) > 0 else 0\n",
    "    cum_non_red_return = ((1 + non_red_returns).prod() - 1) * 100 if len(non_red_returns) > 0 else 0\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nMarket Statistics:\")\n",
    "    print(f\"Correlation between SPY and A/D Line: {correlation:.4f}\")\n",
    "    print(f\"Percentage of days with red background: {red_days_pct:.2f}%\")\n",
    "    print(f\"Average daily SPY return during red periods: {avg_red_return:.4f}%\")\n",
    "    print(f\"Average daily SPY return during non-red periods: {avg_non_red_return:.4f}%\")\n",
    "    print(f\"Cumulative SPY return during red periods: {cum_red_return:.2f}%\")\n",
    "    print(f\"Cumulative SPY return during non-red periods: {cum_non_red_return:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'correlation': correlation,\n",
    "        'red_days_pct': red_days_pct,\n",
    "        'avg_red_return': avg_red_return,\n",
    "        'avg_non_red_return': avg_non_red_return,\n",
    "        'cum_red_return': cum_red_return,\n",
    "        'cum_non_red_return': cum_non_red_return\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Database path\n",
    "    db_path = \"stock_market_data.db\"\n",
    "    \n",
    "    # Run the analysis and plot\n",
    "    results = plot_ad_analysis(db_path)\n",
    "    \n",
    "    # Calculate and display market statistics\n",
    "    if results:\n",
    "        stats = calculate_market_statistics(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trader_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
