{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c9c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ======== CONFIGURABLE PARAMETERS ========\n",
    "# Market data parameters\n",
    "TICKER = 'SPY'  # Main ticker to analyze\n",
    "VIX_TICKER = '^VIX'  # Volatility index\n",
    "TNX_TICKER = '^TNX'  # 10-Year Treasury Yield\n",
    "GLD_TICKER = 'GLD'  # Gold ETF\n",
    "XLY_TICKER = 'XLY'  # Consumer Discretionary ETF\n",
    "XLP_TICKER = 'XLP'  # Consumer Staples ETF\n",
    "XLU_TICKER = 'XLU'  # Utilities ETF\n",
    "XLF_TICKER = 'XLF'  # Financial ETF\n",
    "HYG_TICKER = 'HYG'  # High Yield Corporate Bond ETF\n",
    "TLT_TICKER = 'TLT'  # 20+ Year Treasury Bond ETF\n",
    "VIX3M_TICKER = '^VIX3M'  # 3-Month VIX\n",
    "IRX_TICKER = '^IRX'  # 2-Year Treasury Yield\n",
    "UUP_TICKER = 'UUP'  # US Dollar Index ETF\n",
    "TIP_TICKER = 'TIP'  # TIPS ETF\n",
    "IEF_TICKER = 'IEF'  # 7-10 Year Treasury ETF\n",
    "START_DATE = \"1995-01-01\"  # Historical data start date\n",
    "\n",
    "# HMM model parameters  \n",
    "NUM_REGIMES = 3  # Number of market regimes\n",
    "MAX_ITERATIONS = 1000  # Max iterations for HMM convergence\n",
    "COVARIANCE_TYPE = 'full'  # Options: 'full', 'tied', 'diag', 'spherical'\n",
    "\n",
    "# Technical indicator parameters (keeping same as original)\n",
    "VOL_WINDOW = 21  # Window for volatility calculation (21 days ~ 1 month)\n",
    "MOMENTUM_WINDOW = 63  # Window for momentum calculation (63 days ~ 3 months)\n",
    "SMA_FAST = 20  # Fast moving average\n",
    "SMA_SLOW = 50  # Slow moving average\n",
    "BB_WINDOW = 20  # Bollinger Bands window\n",
    "BB_STD = 2  # Bollinger Bands standard deviation multiplier\n",
    "ATR_WINDOW = 14  # Average True Range window\n",
    "CHOP_WINDOW = 14  # Choppiness Index window\n",
    "SECTOR_WINDOW = 10  # Window for sector rotation indicators\n",
    "CREDIT_MA_WINDOW = 30  # Window for credit spread MA\n",
    "\n",
    "# Training period\n",
    "TRAIN_START_DATE = \"2018-01-05\"\n",
    "TRAIN_END_DATE = \"2025-01-01\"\n",
    "\n",
    "# ======== DATA PREPARATION FUNCTIONS ========\n",
    "\n",
    "def load_ad_line_data(filepath=\"nyse_breadth_2023.csv\"):\n",
    "    \"\"\"Load and prepare NYSE breadth data for A/D line indicators\"\"\"\n",
    "    print(f\"Loading A/D line data from {filepath}...\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    ad_data = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert the date column to datetime\n",
    "    ad_data['Date'] = pd.to_datetime(ad_data['Date'])\n",
    "    \n",
    "    # Calculate basic breadth metrics\n",
    "    # 1. Cumulative A/D Line\n",
    "    ad_data['Net_Advances'] = ad_data['Advancers'] - ad_data['Decliners']\n",
    "    ad_data['AD_Line'] = ad_data['Net_Advances'].cumsum()\n",
    "    \n",
    "    # 2. A/D Ratio and Z-Score\n",
    "    ad_data['AD_Ratio'] = ad_data['Advancers'] / ad_data['Decliners']\n",
    "    ad_data['Log_AD_Ratio'] = np.log(ad_data['AD_Ratio'])\n",
    "    ad_data['Log_AD_Ratio_21d_Mean'] = ad_data['Log_AD_Ratio'].rolling(window=21).mean()\n",
    "    ad_data['Log_AD_Ratio_21d_StdDev'] = ad_data['Log_AD_Ratio'].rolling(window=21).std()\n",
    "    ad_data['AD_Ratio_Z_Score'] = (ad_data['Log_AD_Ratio'] - ad_data['Log_AD_Ratio_21d_Mean']) / ad_data['Log_AD_Ratio_21d_StdDev']\n",
    "    \n",
    "    # 3. McClellan Oscillator\n",
    "    # Calculate EMAs of Net Advances\n",
    "    ad_data['EMA19_Net_Advances'] = ad_data['Net_Advances'].ewm(span=19, adjust=False).mean()\n",
    "    ad_data['EMA39_Net_Advances'] = ad_data['Net_Advances'].ewm(span=39, adjust=False).mean()\n",
    "    ad_data['McClellan_Oscillator'] = ad_data['EMA19_Net_Advances'] - ad_data['EMA39_Net_Advances']\n",
    "    # Normalize by total issues\n",
    "    ad_data['Total_Issues'] = ad_data['Advancers'] + ad_data['Decliners'] + ad_data['Neutral']\n",
    "    ad_data['McClellan_Oscillator_Norm'] = ad_data['McClellan_Oscillator'] / (ad_data['Total_Issues']) * 1000\n",
    "    \n",
    "    # 4. Percentage of Advancing Issues\n",
    "    ad_data['Advancing_Percentage'] = ad_data['Advancers'] / ad_data['Total_Issues'] * 100\n",
    "    ad_data['Advancing_Percentage_10MA'] = ad_data['Advancing_Percentage'].rolling(window=10).mean()\n",
    "    ad_data['Advancing_Percentage_Z'] = (ad_data['Advancing_Percentage'] - ad_data['Advancing_Percentage'].rolling(window=21).mean()) / ad_data['Advancing_Percentage'].rolling(window=21).std()\n",
    "    \n",
    "    # 5. Breadth Thrust Indicator\n",
    "    ad_data['Daily_Thrust'] = ad_data['Advancers'] / (ad_data['Advancers'] + ad_data['Decliners'])\n",
    "    ad_data['Breadth_Thrust'] = ad_data['Daily_Thrust'].ewm(span=10, adjust=False).mean()\n",
    "    ad_data['Thrust_Signal'] = (ad_data['Breadth_Thrust'] > 0.65).astype(int)\n",
    "    \n",
    "    # 6. McClellan Summation Index\n",
    "    ad_data['McClellan_Summation_Index'] = ad_data['McClellan_Oscillator'].cumsum()\n",
    "    ad_data['McClellan_SI_10MA'] = ad_data['McClellan_Summation_Index'].rolling(window=10).mean()\n",
    "    \n",
    "    # 7. A/D Line Momentum\n",
    "    ad_data['AD_Line_5d_ROC'] = ad_data['AD_Line'].pct_change(periods=5) * 100\n",
    "    ad_data['AD_Line_10d_ROC'] = ad_data['AD_Line'].pct_change(periods=10) * 100\n",
    "    ad_data['AD_Line_20d_ROC'] = ad_data['AD_Line'].pct_change(periods=20) * 100\n",
    "    ad_data['AD_Line_Momentum_Z'] = (ad_data['AD_Line_10d_ROC'] - ad_data['AD_Line_10d_ROC'].rolling(window=50).mean()) / ad_data['AD_Line_10d_ROC'].rolling(window=50).std()\n",
    "    \n",
    "    # 8. A/D Line Moving Average Crossovers\n",
    "    ad_data['AD_Line_20MA'] = ad_data['AD_Line'].rolling(window=20).mean()\n",
    "    ad_data['AD_Line_50MA'] = ad_data['AD_Line'].rolling(window=50).mean()\n",
    "    ad_data['AD_Line_Golden_Cross'] = (ad_data['AD_Line_20MA'] > ad_data['AD_Line_50MA']).astype(int)\n",
    "    ad_data['AD_Line_Dist_20MA'] = (ad_data['AD_Line'] / ad_data['AD_Line_20MA'] - 1) * 100\n",
    "    \n",
    "    # 9. Composite Breadth Indicator\n",
    "    # Normalize each component to range [0,1]\n",
    "    ad_data['AD_Ratio_Z_Norm'] = (ad_data['AD_Ratio_Z_Score'].clip(-3, 3) + 3) / 6\n",
    "    ad_data['McClellan_Osc_Norm'] = (ad_data['McClellan_Oscillator'].clip(-150, 150) + 150) / 300\n",
    "    ad_data['Breadth_Thrust_Norm'] = ad_data['Breadth_Thrust']\n",
    "    ad_data['Advancing_Pct_Norm'] = ad_data['Advancing_Percentage'] / 100\n",
    "    \n",
    "    # Create composite\n",
    "    ad_data['Composite_Breadth'] = (ad_data['AD_Ratio_Z_Norm'] + \n",
    "                                   ad_data['McClellan_Osc_Norm'] + \n",
    "                                   ad_data['Breadth_Thrust_Norm'] + \n",
    "                                   ad_data['Advancing_Pct_Norm']) / 4\n",
    "    \n",
    "    ad_data['Composite_Breadth_Z'] = (ad_data['Composite_Breadth'] - \n",
    "                                     ad_data['Composite_Breadth'].rolling(window=50).mean()) / ad_data['Composite_Breadth'].rolling(window=50).std().fillna(1)\n",
    "    \n",
    "    return ad_data\n",
    "\n",
    "def download_market_data(ticker, vix_ticker, tnx_ticker, gld_ticker, xly_ticker, xlp_ticker, xlu_ticker, xlf_ticker, hyg_ticker, tlt_ticker, start_date):\n",
    "    \"\"\"Download and prepare market data\"\"\"\n",
    "    end_date = (datetime.today() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Downloading market data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # Download ticker, VIX, TNX, GLD, sector ETFs, and bond ETF data\n",
    "    df_ticker = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True)\n",
    "    df_vix = yf.download(vix_ticker, start=start_date, end=end_date)\n",
    "    df_tnx = yf.download(tnx_ticker, start=start_date, end=end_date)\n",
    "    df_gld = yf.download(gld_ticker, start=start_date, end=end_date)\n",
    "    df_xly = yf.download(xly_ticker, start=start_date, end=end_date)\n",
    "    df_xlp = yf.download(xlp_ticker, start=start_date, end=end_date)\n",
    "    df_xlu = yf.download(xlu_ticker, start=start_date, end=end_date)\n",
    "    df_xlf = yf.download(xlf_ticker, start=start_date, end=end_date)\n",
    "    df_hyg = yf.download(hyg_ticker, start=start_date, end=end_date)\n",
    "    df_tlt = yf.download(tlt_ticker, start=start_date, end=end_date)\n",
    "    \n",
    "    # Download additional tickers for new features\n",
    "    df_vix3m = yf.download(VIX3M_TICKER, start=start_date, end=end_date)\n",
    "    df_irx = yf.download(IRX_TICKER, start=start_date, end=end_date)\n",
    "    df_uup = yf.download(UUP_TICKER, start=start_date, end=end_date)\n",
    "    df_tip = yf.download(TIP_TICKER, start=start_date, end=end_date)\n",
    "    df_ief = yf.download(IEF_TICKER, start=start_date, end=end_date)\n",
    "    \n",
    "    # Fix column structure and reset index\n",
    "    if len(df_ticker.columns.names) > 1:\n",
    "        df_ticker.columns = df_ticker.columns.droplevel(1)\n",
    "    if len(df_vix.columns.names) > 1:\n",
    "        df_vix.columns = df_vix.columns.droplevel(1)\n",
    "    if len(df_tnx.columns.names) > 1:\n",
    "        df_tnx.columns = df_tnx.columns.droplevel(1)\n",
    "    if len(df_gld.columns.names) > 1:\n",
    "        df_gld.columns = df_gld.columns.droplevel(1)\n",
    "    if len(df_xly.columns.names) > 1:\n",
    "        df_xly.columns = df_xly.columns.droplevel(1)\n",
    "    if len(df_xlp.columns.names) > 1:\n",
    "        df_xlp.columns = df_xlp.columns.droplevel(1)\n",
    "    if len(df_xlu.columns.names) > 1:\n",
    "        df_xlu.columns = df_xlu.columns.droplevel(1)\n",
    "    if len(df_xlf.columns.names) > 1:\n",
    "        df_xlf.columns = df_xlf.columns.droplevel(1)\n",
    "    if len(df_hyg.columns.names) > 1:\n",
    "        df_hyg.columns = df_hyg.columns.droplevel(1)\n",
    "    if len(df_tlt.columns.names) > 1:\n",
    "        df_tlt.columns = df_tlt.columns.droplevel(1)\n",
    "    \n",
    "    # Fix column structure for new tickers\n",
    "    if len(df_vix3m.columns.names) > 1:\n",
    "        df_vix3m.columns = df_vix3m.columns.droplevel(1)\n",
    "    if len(df_irx.columns.names) > 1:\n",
    "        df_irx.columns = df_irx.columns.droplevel(1)\n",
    "    if len(df_uup.columns.names) > 1:\n",
    "        df_uup.columns = df_uup.columns.droplevel(1)\n",
    "    if len(df_tip.columns.names) > 1:\n",
    "        df_tip.columns = df_tip.columns.droplevel(1)\n",
    "    if len(df_ief.columns.names) > 1:\n",
    "        df_ief.columns = df_ief.columns.droplevel(1)\n",
    "    \n",
    "    df_ticker = df_ticker.reset_index()\n",
    "    df_vix = df_vix.reset_index()\n",
    "    df_tnx = df_tnx.reset_index()\n",
    "    df_gld = df_gld.reset_index()\n",
    "    df_xly = df_xly.reset_index()\n",
    "    df_xlp = df_xlp.reset_index()\n",
    "    df_xlu = df_xlu.reset_index()\n",
    "    df_xlf = df_xlf.reset_index()\n",
    "    df_hyg = df_hyg.reset_index()\n",
    "    df_tlt = df_tlt.reset_index()\n",
    "    \n",
    "    df_vix3m = df_vix3m.reset_index()\n",
    "    df_irx = df_irx.reset_index()\n",
    "    df_uup = df_uup.reset_index()\n",
    "    df_tip = df_tip.reset_index()\n",
    "    df_ief = df_ief.reset_index()\n",
    "    \n",
    "    # Keep only Date and Close from VIX\n",
    "    df_vix = df_vix[['Date', 'Close']].rename(columns={'Close': 'VIX'})\n",
    "    \n",
    "    # Keep only Date and Close from TNX\n",
    "    df_tnx = df_tnx[['Date', 'Close']].rename(columns={'Close': 'TNX'})\n",
    "    \n",
    "    # Keep only relevant columns from GLD, sector ETFs, and bond ETFs\n",
    "    df_gld = df_gld[['Date', 'Close']].rename(columns={'Close': 'GLD'})\n",
    "    df_xly = df_xly[['Date', 'Close']].rename(columns={'Close': 'XLY'})\n",
    "    df_xlp = df_xlp[['Date', 'Close']].rename(columns={'Close': 'XLP'})\n",
    "    df_xlu = df_xlu[['Date', 'Close']].rename(columns={'Close': 'XLU'})\n",
    "    df_xlf = df_xlf[['Date', 'Close']].rename(columns={'Close': 'XLF'})\n",
    "    df_hyg = df_hyg[['Date', 'Close']].rename(columns={'Close': 'HYG'})\n",
    "    df_tlt = df_tlt[['Date', 'Close']].rename(columns={'Close': 'TLT'})\n",
    "    \n",
    "    # Keep only relevant columns from new tickers\n",
    "    df_vix3m = df_vix3m[['Date', 'Close']].rename(columns={'Close': 'VIX3M'})\n",
    "    df_irx = df_irx[['Date', 'Close']].rename(columns={'Close': 'IRX'})\n",
    "    df_uup = df_uup[['Date', 'Close']].rename(columns={'Close': 'UUP'})\n",
    "    df_tip = df_tip[['Date', 'Close']].rename(columns={'Close': 'TIP'})\n",
    "    df_ief = df_ief[['Date', 'Close']].rename(columns={'Close': 'IEF'})\n",
    "    \n",
    "    # Merge data\n",
    "    df = pd.merge(df_ticker, df_vix, on='Date', how='left')\n",
    "    df = pd.merge(df, df_tnx, on='Date', how='left')\n",
    "    df = pd.merge(df, df_gld, on='Date', how='left')\n",
    "    df = pd.merge(df, df_xly, on='Date', how='left')\n",
    "    df = pd.merge(df, df_xlp, on='Date', how='left')\n",
    "    df = pd.merge(df, df_xlu, on='Date', how='left')\n",
    "    df = pd.merge(df, df_xlf, on='Date', how='left')\n",
    "    df = pd.merge(df, df_hyg, on='Date', how='left')\n",
    "    df = pd.merge(df, df_tlt, on='Date', how='left')\n",
    "    \n",
    "    # Merge new tickers\n",
    "    df = pd.merge(df, df_vix3m, on='Date', how='left')\n",
    "    df = pd.merge(df, df_irx, on='Date', how='left')\n",
    "    df = pd.merge(df, df_uup, on='Date', how='left')\n",
    "    df = pd.merge(df, df_tip, on='Date', how='left')\n",
    "    df = pd.merge(df, df_ief, on='Date', how='left')\n",
    "    \n",
    "    # Fill missing values with forward fill method\n",
    "    df['VIX'] = df['VIX'].fillna(method='ffill')\n",
    "    df['TNX'] = df['TNX'].fillna(method='ffill')\n",
    "    df['GLD'] = df['GLD'].fillna(method='ffill')\n",
    "    df['XLY'] = df['XLY'].fillna(method='ffill')\n",
    "    df['XLP'] = df['XLP'].fillna(method='ffill')\n",
    "    df['XLU'] = df['XLU'].fillna(method='ffill')\n",
    "    df['XLF'] = df['XLF'].fillna(method='ffill')\n",
    "    df['HYG'] = df['HYG'].fillna(method='ffill')\n",
    "    df['TLT'] = df['TLT'].fillna(method='ffill')\n",
    "    \n",
    "    # Fill missing values for new tickers\n",
    "    df['VIX3M'] = df['VIX3M'].fillna(method='ffill')\n",
    "    df['IRX'] = df['IRX'].fillna(method='ffill')\n",
    "    df['UUP'] = df['UUP'].fillna(method='ffill')\n",
    "    df['TIP'] = df['TIP'].fillna(method='ffill')\n",
    "    df['IEF'] = df['IEF'].fillna(method='ffill')\n",
    "    \n",
    "    # Calculate daily log returns\n",
    "    df['LogVIX'] = np.log(df['VIX'])\n",
    "    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1)) * 100\n",
    "    df['GLD_Log_Return'] = np.log(df['GLD'] / df['GLD'].shift(1)) * 100\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_bollinger_bands(df, window=20, num_std=2):\n",
    "    \"\"\"Calculate Bollinger Bands and related metrics\"\"\"\n",
    "    # Calculate Bollinger Bands\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=window).mean()\n",
    "    rolling_std = df['Close'].rolling(window=window).std()\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (rolling_std * num_std)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (rolling_std * num_std)\n",
    "    \n",
    "    # Calculate Bollinger Band Width (normalized)\n",
    "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle'] * 100\n",
    "    \n",
    "    # Calculate %B (position within the bands)\n",
    "    df['BB_PercentB'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_atr(df, window=14):\n",
    "    \"\"\"Calculate Average True Range (ATR)\"\"\"\n",
    "    # Calculate True Range\n",
    "    df['TR1'] = abs(df['High'] - df['Low'])\n",
    "    df['TR2'] = abs(df['High'] - df['Close'].shift(1))\n",
    "    df['TR3'] = abs(df['Low'] - df['Close'].shift(1))\n",
    "    df['True_Range'] = df[['TR1', 'TR2', 'TR3']].max(axis=1)\n",
    "    \n",
    "    # Calculate ATR using Wilder's smoothing method\n",
    "    df['ATR'] = df['True_Range'].rolling(window=window).mean()\n",
    "    \n",
    "    # Normalize ATR by price\n",
    "    df['ATR_Normalized'] = df['ATR'] / df['Close'] * 100\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    df = df.drop(['TR1', 'TR2', 'TR3'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_choppiness_index(df, window=14):\n",
    "    \"\"\"Calculate Choppiness Index\"\"\"\n",
    "    if 'ATR' not in df.columns:\n",
    "        df = calculate_atr(df, window)\n",
    "    \n",
    "    df['MaxHi'] = df['High'].rolling(window=window).max()\n",
    "    df['MinLo'] = df['Low'].rolling(window=window).min()\n",
    "    df['ATR_Sum'] = df['ATR'].rolling(window=window).sum()\n",
    "    \n",
    "    # Calculate Choppiness Index\n",
    "    df['Choppiness_Index'] = 100 * np.log10(df['ATR_Sum'] / (df['MaxHi'] - df['MinLo'])) / np.log10(window)\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    df = df.drop(['MaxHi', 'MinLo', 'ATR_Sum'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_features(data, ad_data=None):\n",
    "    \"\"\"Calculate features for regime classification, now with A/D line indicators\"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Volatility features\n",
    "    df['Volatility'] = df['Log_Return'].rolling(window=VOL_WINDOW).std() * np.sqrt(252)  # Annualized\n",
    "    df['Volume_Z_Score'] = (df['Volume'] - df['Volume'].rolling(window=VOL_WINDOW).mean()) / df['Volume'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    # Trend features\n",
    "    df['Momentum'] = df['Close'].pct_change(periods=MOMENTUM_WINDOW) * 100\n",
    "    df['SMA_Fast'] = df['Close'].rolling(window=SMA_FAST).mean()\n",
    "    df['SMA_Slow'] = df['Close'].rolling(window=SMA_SLOW).mean()\n",
    "    df['SMA_Ratio'] = df['SMA_Fast'] / df['SMA_Slow']\n",
    "    \n",
    "    # Price distance from moving averages\n",
    "    df['Price_to_SMA_Fast'] = df['Close'] / df['SMA_Fast'] - 1\n",
    "    df['Price_to_SMA_Slow'] = df['Close'] / df['SMA_Slow'] - 1\n",
    "    \n",
    "    # VIX-based features\n",
    "    df['VIX_Ratio'] = df['VIX'] / df['VIX'].rolling(window=VOL_WINDOW).mean()\n",
    "    df['VIX_Change'] = df['VIX'].pct_change(periods=5) * 100  # 5-day VIX change\n",
    "    \n",
    "    # Treasury Yield (TNX) features\n",
    "    df['TNX_Level'] = df['TNX']  # Absolute yield level\n",
    "    df['TNX_Daily_Change'] = df['TNX'].diff() * 100  # Daily change in basis points\n",
    "    df['TNX_Weekly_Change'] = df['TNX'].diff(5) * 100  # 1-week change in basis points\n",
    "    df['TNX_Z_Score'] = (df['TNX'] - df['TNX'].rolling(window=VOL_WINDOW).mean()) / df['TNX'].rolling(window=VOL_WINDOW).std()\n",
    "    df['TNX_Ratio'] = df['TNX'] / df['TNX'].rolling(window=VOL_WINDOW).mean()\n",
    "    \n",
    "    # Gold (GLD) features\n",
    "    if df['GLD'].notna().any():\n",
    "        df['GLD_Volatility'] = df['GLD_Log_Return'].rolling(window=VOL_WINDOW).std() * np.sqrt(252)\n",
    "        df['GLD_SPY_Ratio'] = df['GLD'] / df['Close']\n",
    "        df['GLD_SPY_Ratio_Change'] = df['GLD_SPY_Ratio'].pct_change(periods=5) * 100\n",
    "        df['GLD_Z_Score'] = (df['GLD'] - df['GLD'].rolling(window=VOL_WINDOW).mean()) / df['GLD'].rolling(window=VOL_WINDOW).std()\n",
    "        df['GLD_Momentum'] = df['GLD'].pct_change(periods=MOMENTUM_WINDOW) * 100\n",
    "        df['GLD_SPY_Momentum_Diff'] = df['GLD_Momentum'] - df['Momentum']\n",
    "    \n",
    "    # Sector Rotation indicators\n",
    "    if df['XLY'].notna().any() and df['XLP'].notna().any():\n",
    "        df['XLY_XLP_Ratio'] = df['XLY'] / df['XLP']\n",
    "        df['XLY_XLP_Change'] = df['XLY_XLP_Ratio'].pct_change(periods=SECTOR_WINDOW) * 100\n",
    "        df['XLY_XLP_Z'] = (df['XLY_XLP_Ratio'] - df['XLY_XLP_Ratio'].rolling(window=VOL_WINDOW).mean()) / df['XLY_XLP_Ratio'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    if df['XLU'].notna().any():\n",
    "        df['XLU_SPY_Ratio'] = df['XLU'] / df['Close']\n",
    "        df['XLU_SPY_Change'] = df['XLU_SPY_Ratio'].pct_change(periods=SECTOR_WINDOW) * 100\n",
    "        df['XLU_SPY_Z'] = (df['XLU_SPY_Ratio'] - df['XLU_SPY_Ratio'].rolling(window=VOL_WINDOW).mean()) / df['XLU_SPY_Ratio'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    if df['XLF'].notna().any():\n",
    "        df['XLF_SPY_Ratio'] = df['XLF'] / df['Close']\n",
    "        df['XLF_SPY_Change'] = df['XLF_SPY_Ratio'].pct_change(periods=SECTOR_WINDOW) * 100\n",
    "        df['XLF_SPY_Z'] = (df['XLF_SPY_Ratio'] - df['XLF_SPY_Ratio'].rolling(window=VOL_WINDOW).mean()) / df['XLF_SPY_Ratio'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    # Credit Spread indicators (HYG/TLT)\n",
    "    if df['HYG'].notna().any() and df['TLT'].notna().any():\n",
    "        df['HYG_TLT_Ratio'] = df['HYG'] / df['TLT']\n",
    "        df['HYG_TLT_Daily_Change'] = df['HYG_TLT_Ratio'].pct_change() * 100\n",
    "        df['HYG_TLT_MA'] = df['HYG_TLT_Ratio'].rolling(window=CREDIT_MA_WINDOW).mean()\n",
    "        df['HYG_TLT_MA_Diff'] = (df['HYG_TLT_Ratio'] / df['HYG_TLT_MA'] - 1) * 100\n",
    "        df['HYG_TLT_Z'] = (df['HYG_TLT_Ratio'] - df['HYG_TLT_Ratio'].rolling(window=VOL_WINDOW).mean()) / df['HYG_TLT_Ratio'].rolling(window=VOL_WINDOW).std()\n",
    "        df['HYG_TLT_Volatility'] = df['HYG_TLT_Daily_Change'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    # Volume features\n",
    "    df['Volume_Trend'] = df['Volume'] / df['Volume'].rolling(window=VOL_WINDOW).mean()\n",
    "    df['Volume_to_Volatility'] = df['Volume'] / (df['Volatility'] + 1e-10)\n",
    "    \n",
    "    # Combined features\n",
    "    df['Return_Volatility_Ratio'] = df['Log_Return'] / (df['Volatility'] + 1e-10)\n",
    "    df['VIX_Volatility_Ratio'] = df['VIX'] / (df['Volatility'] + 1e-10)\n",
    "    \n",
    "    # Add Bollinger Bands features\n",
    "    df = calculate_bollinger_bands(df, window=BB_WINDOW, num_std=BB_STD)\n",
    "    \n",
    "    # Add ATR features\n",
    "    df = calculate_atr(df, window=ATR_WINDOW)\n",
    "    \n",
    "    # Add Choppiness Index\n",
    "    df = calculate_choppiness_index(df, window=CHOP_WINDOW)\n",
    "    \n",
    "    # Calculate Z-scores for the new indicators\n",
    "    df['BB_Width_Z'] = (df['BB_Width'] - df['BB_Width'].rolling(window=VOL_WINDOW).mean()) / df['BB_Width'].rolling(window=VOL_WINDOW).std()\n",
    "    df['ATR_Norm_Z'] = (df['ATR_Normalized'] - df['ATR_Normalized'].rolling(window=VOL_WINDOW).mean()) / df['ATR_Normalized'].rolling(window=VOL_WINDOW).std()\n",
    "    df['Choppiness_Z'] = (df['Choppiness_Index'] - df['Choppiness_Index'].rolling(window=VOL_WINDOW).mean()) / df['Choppiness_Index'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    # Add the new features requested by the user\n",
    "    \n",
    "    # 1. Yield Curve Spread (10Y-2Y)\n",
    "    if 'TNX' in df.columns and 'IRX' in df.columns:\n",
    "        # Calculate the spread in percentage points\n",
    "        df['Yield_Curve_Spread'] = df['TNX'] - df['IRX']\n",
    "        \n",
    "        # Calculate Z-score of the spread\n",
    "        df['Yield_Curve_Spread_Z'] = (df['Yield_Curve_Spread'] - df['Yield_Curve_Spread'].rolling(window=VOL_WINDOW).mean()) / df['Yield_Curve_Spread'].rolling(window=VOL_WINDOW).std()\n",
    "        \n",
    "        # Calculate weekly change in the spread\n",
    "        df['Yield_Curve_Spread_Change'] = df['Yield_Curve_Spread'].diff(5) * 100  # 1-week change in basis points\n",
    "        \n",
    "        # Calculate inversion flag (1 if inverted, 0 if normal)\n",
    "        df['Yield_Curve_Inverted'] = (df['Yield_Curve_Spread'] < 0).astype(int)\n",
    "    \n",
    "    # 2. VIX/VIX3M Ratio (Volatility Term Structure)\n",
    "    if 'VIX' in df.columns and 'VIX3M' in df.columns:\n",
    "        # Calculate ratio (values < 1 indicate normal contango, values > 1 indicate backwardation/fear)\n",
    "        df['VIX_VIX3M_Ratio'] = df['VIX'] / df['VIX3M']\n",
    "        \n",
    "        # Calculate Z-score of the ratio\n",
    "        df['VIX_VIX3M_Ratio_Z'] = (df['VIX_VIX3M_Ratio'] - df['VIX_VIX3M_Ratio'].rolling(window=VOL_WINDOW).mean()) / df['VIX_VIX3M_Ratio'].rolling(window=VOL_WINDOW).std()\n",
    "        \n",
    "        # Calculate spread (for alternative representation)\n",
    "        df['VIX_VIX3M_Spread'] = df['VIX'] - df['VIX3M']\n",
    "        \n",
    "        # Flag for backwardation (1 if in backwardation, 0 if in contango)\n",
    "        df['VIX_Backwardation'] = (df['VIX_VIX3M_Ratio'] > 1).astype(int)\n",
    "    \n",
    "    # 3. UUP Z-Score (Dollar Strength)\n",
    "    if 'UUP' in df.columns:\n",
    "        # Calculate UUP returns\n",
    "        df['UUP_Return'] = df['UUP'].pct_change() * 100\n",
    "        \n",
    "        # Calculate UUP Z-score (normalized dollar strength)\n",
    "        df['UUP_Z_Score'] = (df['UUP'] - df['UUP'].rolling(window=VOL_WINDOW).mean()) / df['UUP'].rolling(window=VOL_WINDOW).std()\n",
    "        \n",
    "        # Calculate UUP momentum\n",
    "        df['UUP_Momentum'] = df['UUP'].pct_change(periods=VOL_WINDOW) * 100\n",
    "        \n",
    "        # Calculate UUP volatility\n",
    "        df['UUP_Volatility'] = df['UUP_Return'].rolling(window=VOL_WINDOW).std() * np.sqrt(252)\n",
    "    \n",
    "    # 4. TIP/IEF Ratio (Inflation Expectations)\n",
    "    if 'TIP' in df.columns and 'IEF' in df.columns:\n",
    "        # Calculate the ratio\n",
    "        df['TIP_IEF_Ratio'] = df['TIP'] / df['IEF']\n",
    "        \n",
    "        # Calculate Z-score of the ratio\n",
    "        df['TIP_IEF_Ratio_Z'] = (df['TIP_IEF_Ratio'] - df['TIP_IEF_Ratio'].rolling(window=VOL_WINDOW).mean()) / df['TIP_IEF_Ratio'].rolling(window=VOL_WINDOW).std()\n",
    "        \n",
    "        # Calculate weekly change in the ratio\n",
    "        df['TIP_IEF_Ratio_Change'] = df['TIP_IEF_Ratio'].pct_change(periods=5) * 100  # 5-day change\n",
    "        \n",
    "        # Calculate TIP performance vs SPY\n",
    "        df['TIP_SPY_Ratio'] = df['TIP'] / df['Close']\n",
    "        df['TIP_SPY_Ratio_Z'] = (df['TIP_SPY_Ratio'] - df['TIP_SPY_Ratio'].rolling(window=VOL_WINDOW).mean()) / df['TIP_SPY_Ratio'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    # ADD NEW A/D LINE FEATURES\n",
    "    if ad_data is not None:\n",
    "        # Merge A/D line data with price data\n",
    "        ad_data_subset = ad_data[['Date', 'AD_Line', 'AD_Ratio_Z_Score', 'McClellan_Oscillator', \n",
    "                                 'McClellan_Oscillator_Norm', 'Advancing_Percentage_Z',\n",
    "                                 'Breadth_Thrust', 'McClellan_Summation_Index',\n",
    "                                 'AD_Line_Momentum_Z', 'AD_Line_Dist_20MA', \n",
    "                                 'Composite_Breadth_Z', 'AD_Line_Golden_Cross']]\n",
    "        \n",
    "        # Convert to datetime to ensure proper merging\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        ad_data_subset['Date'] = pd.to_datetime(ad_data_subset['Date'])\n",
    "        \n",
    "        # Merge on Date\n",
    "        df = pd.merge(df, ad_data_subset, on='Date', how='left')\n",
    "        \n",
    "        # Calculate AD Line Divergence with Price\n",
    "        if 'AD_Line' in df.columns:\n",
    "            # Calculate rates of change for AD Line and Price\n",
    "            df['AD_Line_20d_ROC'] = df['AD_Line'].pct_change(periods=20) * 100\n",
    "            df['Price_20d_ROC'] = df['Close'].pct_change(periods=20) * 100\n",
    "            \n",
    "            # Calculate divergence spread\n",
    "            df['AD_Price_Divergence'] = df['AD_Line_20d_ROC'] - df['Price_20d_ROC']\n",
    "            \n",
    "            # Calculate correlation over 30-day window\n",
    "            df['AD_Price_Correlation'] = df['AD_Line'].rolling(window=30).corr(df['Close'])\n",
    "            \n",
    "            # Flag negative divergences (AD_Line not confirming price highs)\n",
    "            price_new_high = df['Close'] > df['Close'].rolling(window=20).max().shift(1)\n",
    "            ad_line_not_confirming = df['AD_Line'] < df['AD_Line'].rolling(window=20).max().shift(1)\n",
    "            df['AD_Negative_Divergence'] = (price_new_high & ad_line_not_confirming).astype(int)\n",
    "    \n",
    "    # Fill NaN values and drop remaining NaN rows\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ======== FEATURE SELECTION & DIMENSIONALITY REDUCTION ========\n",
    "def select_and_prepare_features(data, n_components=5):\n",
    "    \"\"\"\n",
    "    Select key features and reduce dimensionality using PCA\n",
    "    \n",
    "    This step is critical for HMM as it works better with fewer dimensions\n",
    "    \"\"\"\n",
    "    # Updated feature selection based on requested changes plus new A/D line features\n",
    "    selected_features = [\n",
    "        'Log_Return',              # Daily returns\n",
    "        'Volatility',              # Recent volatility\n",
    "        'Price_to_SMA_Fast',       # Short-term trend\n",
    "        'Price_to_SMA_Slow',       # Long-term trend\n",
    "        'Momentum',                # Medium-term price momentum\n",
    "        'VIX_Ratio',               # VIX relative to its recent average (kept - removed VIX_Z_Score)\n",
    "        'Volume_Z_Score',          # Volume relative to its recent history\n",
    "        'BB_Width_Z',              # Normalized Bollinger Band width\n",
    "        'ATR_Norm_Z',              # Normalized ATR\n",
    "        'Choppiness_Z',            # Market choppiness/trendiness\n",
    "        'TNX_Z_Score',             # 10-Year yield relative to recent history\n",
    "        'GLD_SPY_Ratio_Change',    # Gold/SPY ratio change\n",
    "        'XLY_XLP_Z',               # Discretionary vs. Staples (kept - removed XLP_SPY_Z)\n",
    "        'HYG_TLT_Z',               # Credit spread\n",
    "        'XLP_SPY_Z',               # Staples vs. SPY\n",
    "        'Yield_Curve_Spread_Z',    # NEW: Yield curve spread (10Y-2Y) Z-score\n",
    "        'VIX_VIX3M_Ratio',         # NEW: VIX/VIX3M ratio\n",
    "        'UUP_Z_Score',             # NEW: UUP (Dollar) strength Z-score\n",
    "        'TIP_IEF_Ratio_Z',         # NEW: TIP/IEF (inflation expectations) Z-score\n",
    "        # NEW A/D LINE FEATURES\n",
    "        'AD_Ratio_Z_Score',        # A/D Ratio Z-Score\n",
    "        'McClellan_Oscillator_Norm', # Normalized McClellan Oscillator\n",
    "        'Advancing_Percentage_Z',  # Z-Score of % Advancing Issues\n",
    "        'Breadth_Thrust',          # Breadth Thrust Indicator\n",
    "        'AD_Line_Momentum_Z',      # Z-Score of A/D Line momentum\n",
    "        'AD_Price_Divergence',     # Divergence between A/D Line and Price\n",
    "        'Composite_Breadth_Z'      # Z-Score of Composite Breadth Indicator\n",
    "    ]\n",
    "    \n",
    "    # Check if any feature has all NaN values and remove it\n",
    "    valid_features = []\n",
    "    for feature in selected_features:\n",
    "        if feature in data.columns and not data[feature].isna().all():\n",
    "            valid_features.append(feature)\n",
    "        elif feature in data.columns:\n",
    "            print(f\"Warning: Feature '{feature}' contains all NaN values and will be excluded\")\n",
    "        else:\n",
    "            print(f\"Warning: Feature '{feature}' not found in dataset\")\n",
    "    \n",
    "    selected_features = valid_features\n",
    "    feature_data = data[selected_features].values\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Reduce dimensionality with PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Print explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cum_explained_variance = np.cumsum(explained_variance)\n",
    "    print(f\"PCA explained variance: {explained_variance}\")\n",
    "    print(f\"Cumulative explained variance: {cum_explained_variance}\")\n",
    "    \n",
    "    # Print feature importance in each component\n",
    "    print(\"\\nPCA Component Loadings:\")\n",
    "    component_df = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "        index=selected_features\n",
    "    )\n",
    "    print(component_df.abs().sort_values(by='PC1', ascending=False))\n",
    "    \n",
    "    return reduced_features, scaler, pca, selected_features\n",
    "\n",
    "# ======== HMM MODEL FUNCTIONS ========\n",
    "def train_hmm_model(data, start_date, end_date, n_components=3, max_iter=1000, cov_type='full'):\n",
    "    \"\"\"Train HMM model on selected features with proper initialization\"\"\"\n",
    "    # Filter data to training period\n",
    "    training = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)].copy()\n",
    "    \n",
    "    print(f\"Training HMM model on data from {start_date} to {end_date}\")\n",
    "    print(f\"Training data shape: {training.shape}\")\n",
    "    \n",
    "    # Reduce dimensionality \n",
    "    # Use fewer components (3-5) as HMM works better with lower dimensions\n",
    "    reduced_features, scaler, pca, selected_features = select_and_prepare_features(training, n_components=4)\n",
    "    \n",
    "    # Initialize HMM with domain knowledge\n",
    "    # Key innovation: Use informed starting values rather than random\n",
    "    \n",
    "    # Initialize transition matrix with high self-transition probabilities\n",
    "    # This enforces regime persistence\n",
    "    transition_matrix = np.array([\n",
    "        [0.97, 0.02, 0.01],  # 97% chance to stay in bull\n",
    "        [0.03, 0.95, 0.02],  # 95% chance to stay in neutral\n",
    "        [0.02, 0.03, 0.95]   # 95% chance to stay in bear\n",
    "    ])\n",
    "    \n",
    "    # Initialize starting probabilities - assume we start in bull market\n",
    "    startprob = np.array([0.7, 0.2, 0.1])\n",
    "    \n",
    "    # Initialize means for each regime\n",
    "    # Component 0: Bull regime (positive returns, low volatility)\n",
    "    # Component 1: Neutral regime (flat returns, moderate volatility)\n",
    "    # Component 2: Bear regime (negative returns, high volatility)\n",
    "    \n",
    "    # First calculate simple statistics to guide initialization\n",
    "    # Use k-means or simple threshold-based approach to set initial means\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_components, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(reduced_features)\n",
    "    \n",
    "    # Calculate mean returns for each cluster\n",
    "    mean_returns = []\n",
    "    for i in range(n_components):\n",
    "        cluster_mask = (cluster_labels == i)\n",
    "        if np.sum(cluster_mask) > 0:\n",
    "            mean_return = np.mean(training.loc[cluster_mask, 'Log_Return'])\n",
    "            mean_returns.append((i, mean_return))\n",
    "    \n",
    "    # Sort clusters by mean returns\n",
    "    mean_returns.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Map from sorted returns to regime index\n",
    "    cluster_to_regime = {}\n",
    "    regime_labels = ['Bull', 'Neutral', 'Bear']\n",
    "    \n",
    "    # Assign highest returns to Bull, lowest to Bear, middle to Neutral\n",
    "    for idx, (cluster, _) in enumerate(mean_returns):\n",
    "        if idx == 0:\n",
    "            cluster_to_regime[cluster] = 0  # Bull\n",
    "        elif idx == len(mean_returns) - 1:\n",
    "            cluster_to_regime[cluster] = 2  # Bear\n",
    "        else:\n",
    "            cluster_to_regime[cluster] = 1  # Neutral\n",
    "    \n",
    "    # Calculate initial means from kmeans\n",
    "    initial_means = np.zeros((n_components, reduced_features.shape[1]))\n",
    "    for i in range(n_components):\n",
    "        for cluster, regime in cluster_to_regime.items():\n",
    "            if regime == i:\n",
    "                mask = (cluster_labels == cluster)\n",
    "                if np.sum(mask) > 0:\n",
    "                    initial_means[i] = np.mean(reduced_features[mask], axis=0)\n",
    "    \n",
    "    # Create the HMM model\n",
    "    model = hmm.GaussianHMM(\n",
    "        n_components=n_components,\n",
    "        covariance_type=cov_type,\n",
    "        n_iter=max_iter,\n",
    "        tol=1e-6,\n",
    "        init_params=\"\",  # Don't initialize params, we'll set them manually\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Set initial parameters based on domain knowledge\n",
    "    model.startprob_ = startprob\n",
    "    model.transmat_ = transition_matrix\n",
    "    model.means_ = initial_means\n",
    "    \n",
    "    # For covariances, we'll initialize with the empirical covariances of the clusters\n",
    "    if cov_type in ['full', 'tied']:\n",
    "        covs = []\n",
    "        for i in range(n_components):\n",
    "            for cluster, regime in cluster_to_regime.items():\n",
    "                if regime == i:\n",
    "                    mask = (cluster_labels == cluster)\n",
    "                    if np.sum(mask) > 0:\n",
    "                        cov = np.cov(reduced_features[mask].T)\n",
    "                        # Ensure numerical stability\n",
    "                        cov += np.eye(cov.shape[0]) * 1e-6\n",
    "                        covs.append(cov)\n",
    "        \n",
    "        if cov_type == 'full':\n",
    "            model.covars_ = np.array(covs)\n",
    "        else:  # tied\n",
    "            model.covars_ = np.mean(covs, axis=0)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(reduced_features)\n",
    "    \n",
    "    # Get state sequence and state probabilities using the Forward-Backward algorithm\n",
    "    hidden_states = model.predict(reduced_features)\n",
    "    state_probs = model.predict_proba(reduced_features)\n",
    "    \n",
    "    # Extract more detailed state probabilities using the Forward-Backward algorithm\n",
    "    # This gives smoother probabilities than just predict_proba\n",
    "    _, forwback = model.score_samples(reduced_features)\n",
    "    \n",
    "    # Map states to regime labels based on behavior\n",
    "    # We'll analyze the characteristics of each state to determine labels\n",
    "    regime_stats = {}\n",
    "    regime_labels = [\"\"] * n_components\n",
    "    \n",
    "    # Analyze the characteristics of each regime\n",
    "    for i in range(n_components):\n",
    "        regime_mask = (hidden_states == i)\n",
    "        if np.sum(regime_mask) > 0:\n",
    "            regime_stats[i] = {\n",
    "                'count': np.sum(regime_mask),\n",
    "                'return_avg': np.mean(training.loc[regime_mask, 'Log_Return']),\n",
    "                'return_std': np.std(training.loc[regime_mask, 'Log_Return']),\n",
    "                'volatility_avg': np.mean(training.loc[regime_mask, 'Volatility']),\n",
    "                'vix_avg': np.mean(training.loc[regime_mask, 'VIX']),\n",
    "                'vix_ratio_avg': np.mean(training.loc[regime_mask, 'VIX_Ratio']),\n",
    "                'momentum_avg': np.mean(training.loc[regime_mask, 'Momentum']),\n",
    "                'prob_avg': np.mean(state_probs[:, i])\n",
    "            }\n",
    "            \n",
    "            # Add more stats if available\n",
    "            for feature in ['BB_Width', 'ATR_Normalized', 'Choppiness_Index', 'TNX', \n",
    "                           'GLD_SPY_Ratio', 'XLY_XLP_Ratio', 'HYG_TLT_Ratio']:\n",
    "                if feature in training.columns:\n",
    "                    regime_stats[i][f'{feature.lower()}_avg'] = np.mean(training.loc[regime_mask, feature])\n",
    "    \n",
    "    # Determine regime labels based on average returns and volatility\n",
    "    # This is a more robust method than just using the state number\n",
    "    return_avgs = [regime_stats[i]['return_avg'] if i in regime_stats else 0 for i in range(n_components)]\n",
    "    volatility_avgs = [regime_stats[i]['volatility_avg'] if i in regime_stats else 0 for i in range(n_components)]\n",
    "    \n",
    "    # Assign labels: highest returns = Bull, lowest returns = Bear, middle = Neutral\n",
    "    # We also consider volatility as a secondary factor\n",
    "    state_characteristics = [(i, return_avgs[i], volatility_avgs[i]) for i in range(n_components)]\n",
    "    \n",
    "    # Sort by returns (highest to lowest)\n",
    "    state_characteristics.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Assign labels\n",
    "    for rank, (state, _, _) in enumerate(state_characteristics):\n",
    "        if rank == 0:\n",
    "            regime_labels[state] = 'Bull'\n",
    "        elif rank == n_components - 1:\n",
    "            regime_labels[state] = 'Bear'\n",
    "        else:\n",
    "            regime_labels[state] = 'Neutral'\n",
    "    \n",
    "    # Print key regime statistics\n",
    "    print(\"\\nRegime Characteristics Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Regime':<8} {'Label':<8} {'Count':<8} {'Return %':<10} {'Vol':<8} {'VIX':<8} {'Momentum':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        if i in regime_stats:\n",
    "            stats = regime_stats[i]\n",
    "            print(f\"{i:<8} {regime_labels[i]:<8} {stats['count']:<8} \"\n",
    "                  f\"{stats['return_avg']:<10.2f} {stats['volatility_avg']:<8.2f} {stats['vix_avg']:<8.2f} \"\n",
    "                  f\"{stats['momentum_avg']:<10.2f}\")\n",
    "    \n",
    "    # Calculate transition probability matrix from the Markov model\n",
    "    transition_matrix = model.transmat_\n",
    "    \n",
    "    # Print transition matrix\n",
    "    print(\"\\nRegime Transition Matrix (daily probabilities):\")    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'From/To':<10}\", end=\"\")\n",
    "    for i in range(n_components):\n",
    "        print(f\"{regime_labels[i]:<10}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        print(f\"{regime_labels[i]:<10}\", end=\"\")\n",
    "        for j in range(n_components):\n",
    "            print(f\"{transition_matrix[i, j]:<10.4f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate expected regime duration in days\n",
    "    print(\"\\nExpected Regime Duration:\")\n",
    "    for i in range(n_components):\n",
    "        duration = 1 / (1 - transition_matrix[i, i]) if transition_matrix[i, i] < 1 else float('inf')\n",
    "        print(f\"{regime_labels[i]:<10} {duration:<10.1f} days\")\n",
    "    \n",
    "    # Calculate AIC and BIC for model evaluation\n",
    "    print(f\"\\nModel Evaluation:\")\n",
    "    # Calculate number of parameters\n",
    "    n_states = model.n_components\n",
    "    n_features = model.means_.shape[1]\n",
    "    \n",
    "    # Number of parameters depends on covariance type\n",
    "    if cov_type == 'full':\n",
    "        n_cov_params = n_states * n_features * (n_features + 1) // 2\n",
    "    elif cov_type == 'diag':\n",
    "        n_cov_params = n_states * n_features\n",
    "    elif cov_type == 'tied':\n",
    "        n_cov_params = n_features * (n_features + 1) // 2\n",
    "    elif cov_type == 'spherical':\n",
    "        n_cov_params = n_states\n",
    "    \n",
    "    # Calculate total number of parameters\n",
    "    n_params = n_states - 1  # startprob_\n",
    "    n_params += n_states * (n_states - 1)  # transmat_\n",
    "    n_params += n_states * n_features  # means_\n",
    "    n_params += n_cov_params  # covars_\n",
    "    \n",
    "    # Calculate AIC and BIC\n",
    "    model_score = model.score(reduced_features)\n",
    "    aic = 2 * n_params - 2 * model_score\n",
    "    bic = np.log(len(reduced_features)) * n_params - 2 * model_score\n",
    "    \n",
    "    print(f\"AIC: {aic:.2f} (lower is better)\")\n",
    "    print(f\"BIC: {bic:.2f} (lower is better)\")\n",
    "    print(f\"Log-likelihood: {model_score:.2f}\")\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "    \n",
    "    # Add regime classifications to the training data\n",
    "    training['Predicted_Regime'] = hidden_states\n",
    "    training['Regime_Label'] = [regime_labels[s] for s in hidden_states]\n",
    "    training['Regime_Probability'] = np.max(state_probs, axis=1)\n",
    "    \n",
    "    # Add probability columns for each regime\n",
    "    for i, label in enumerate(regime_labels):\n",
    "        training[f'Prob_{label}'] = state_probs[:, i]\n",
    "    \n",
    "    # Add smoothed probabilities from forward-backward algorithm\n",
    "    for i, label in enumerate(regime_labels):\n",
    "        training[f'Smooth_Prob_{label}'] = forwback[:, i]\n",
    "    \n",
    "    # Create a bundle of model components for prediction\n",
    "    model_bundle = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'pca': pca,\n",
    "        'selected_features': selected_features,\n",
    "        'regime_labels': regime_labels,\n",
    "        'n_components': n_components\n",
    "    }\n",
    "    \n",
    "    return model_bundle, training, hidden_states, regime_labels, regime_stats\n",
    "\n",
    "def predict_regimes(model_bundle, data, start_date, end_date):\n",
    "    \"\"\"Predict market regimes for a specific date range using the trained HMM model\"\"\"\n",
    "    # Extract model components\n",
    "    model = model_bundle['model']\n",
    "    scaler = model_bundle['scaler']\n",
    "    pca = model_bundle['pca']\n",
    "    selected_features = model_bundle['selected_features']\n",
    "    regime_labels = model_bundle['regime_labels']\n",
    "    \n",
    "    # Filter data for prediction period\n",
    "    pred_data = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)].copy()\n",
    "    \n",
    "    if len(pred_data) == 0:\n",
    "        print(f\"No data available for period {start_date} to {end_date}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    feature_data = pred_data[selected_features].values\n",
    "    \n",
    "    # Scale features using the same scaler as training\n",
    "    scaled_features = scaler.transform(feature_data)\n",
    "    \n",
    "    # Reduce dimensions using PCA\n",
    "    reduced_features = pca.transform(scaled_features)\n",
    "    \n",
    "    # Predict regimes and probabilities\n",
    "    hidden_states = model.predict(reduced_features)\n",
    "    state_probs = model.predict_proba(reduced_features)\n",
    "    \n",
    "    # For smoother predictions, use Forward-Backward algorithm\n",
    "    _, forwback = model.score_samples(reduced_features)\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    pred_data['Predicted_Regime'] = hidden_states\n",
    "    pred_data['Regime_Label'] = [regime_labels[s] for s in hidden_states]\n",
    "    pred_data['Regime_Probability'] = np.max(state_probs, axis=1)\n",
    "    \n",
    "    # Add probability columns for each regime\n",
    "    for i, label in enumerate(regime_labels):\n",
    "        pred_data[f'Prob_{label}'] = state_probs[:, i]\n",
    "        pred_data[f'Smooth_Prob_{label}'] = forwback[:, i]\n",
    "    \n",
    "    # Print basic statistics about the prediction\n",
    "    print(f\"Predicted regimes for period {start_date} to {end_date}\")\n",
    "    \n",
    "    # Calculate regime distribution\n",
    "    regime_counts = pd.Series(hidden_states).value_counts(normalize=True) * 100\n",
    "    print(\"\\nRegime Distribution:\")\n",
    "    for regime, percentage in sorted(regime_counts.items()):\n",
    "        print(f\"Regime {regime} [{regime_labels[regime]}]: {percentage:.2f}%\")\n",
    "    \n",
    "    # Calculate average regime duration\n",
    "    regime_changes = (pred_data['Predicted_Regime'] != pred_data['Predicted_Regime'].shift(1)).sum()\n",
    "    avg_duration = len(pred_data) / (regime_changes if regime_changes > 0 else 1)\n",
    "    print(f\"\\nRegime persistence: {avg_duration:.2f} days average duration\")\n",
    "    \n",
    "    return pred_data\n",
    "\n",
    "def apply_viterbi_smoothing(model, data):\n",
    "    \"\"\"\n",
    "    Apply the Viterbi algorithm to get the most likely state sequence\n",
    "    This can reduce regime switching compared to standard prediction\n",
    "    \"\"\"\n",
    "    # Get the most likely sequence of hidden states using Viterbi algorithm\n",
    "    state_sequence = model.predict(data)\n",
    "    return state_sequence\n",
    "\n",
    "def apply_transition_constraints(predictions, min_duration=10):\n",
    "    \"\"\"\n",
    "    Post-process regime predictions to enforce minimum duration\n",
    "    This reduces frequent regime switching\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions: array of predicted regime states\n",
    "    min_duration: minimum number of days a regime should persist\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    array of smoothed regime states\n",
    "    \"\"\"\n",
    "    smoothed = predictions.copy()\n",
    "    \n",
    "    # Find regime transitions\n",
    "    transitions = np.where(smoothed[1:] != smoothed[:-1])[0] + 1\n",
    "    \n",
    "    # Add start and end points to transitions\n",
    "    transitions = np.insert(transitions, 0, 0)\n",
    "    transitions = np.append(transitions, len(smoothed))\n",
    "    \n",
    "    # Check each segment between transitions\n",
    "    for i in range(len(transitions) - 1):\n",
    "        start = transitions[i]\n",
    "        end = transitions[i+1]\n",
    "        segment_length = end - start\n",
    "        \n",
    "        # If segment is too short, merge with adjacent regimes\n",
    "        if segment_length < min_duration:\n",
    "            # Determine which adjacent segment to merge with\n",
    "            # Choose the one with higher regime probability if available\n",
    "            if i == 0:  # First segment\n",
    "                smoothed[start:end] = smoothed[end]\n",
    "            elif i == len(transitions) - 2:  # Last segment\n",
    "                smoothed[start:end] = smoothed[start-1]\n",
    "            else:\n",
    "                # Check which adjacent segment is longer\n",
    "                prev_segment_length = start - transitions[i-1]\n",
    "                next_segment_length = transitions[i+2] - end if i+2 < len(transitions) else float('inf')\n",
    "                \n",
    "                if prev_segment_length > next_segment_length:\n",
    "                    smoothed[start:end] = smoothed[start-1]\n",
    "                else:\n",
    "                    smoothed[start:end] = smoothed[end]\n",
    "    \n",
    "    return smoothed\n",
    "\n",
    "def calculate_conditional_metrics(data, regime_column='Regime_Label'):\n",
    "    \"\"\"\n",
    "    Calculate conditional performance metrics for each regime\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: DataFrame with regime classifications and return data\n",
    "    regime_column: column name containing regime labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with performance metrics by regime\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    for regime in data[regime_column].unique():\n",
    "        regime_data = data[data[regime_column] == regime]\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(regime_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Calculate daily return statistics\n",
    "        daily_return_mean = regime_data['Log_Return'].mean()\n",
    "        daily_return_std = regime_data['Log_Return'].std()\n",
    "        \n",
    "        # Calculate hit rate (% of positive days)\n",
    "        hit_rate = (regime_data['Log_Return'] > 0).mean() * 100\n",
    "        \n",
    "        # Calculate maximum drawdown\n",
    "        regime_data['Cumulative_Return'] = (1 + regime_data['Log_Return'] / 100).cumprod()\n",
    "        rolling_max = regime_data['Cumulative_Return'].cummax()\n",
    "        drawdown = (regime_data['Cumulative_Return'] / rolling_max - 1) * 100\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Calculate annualized metrics\n",
    "        trading_days_per_year = 252\n",
    "        annualized_return = daily_return_mean * trading_days_per_year\n",
    "        annualized_vol = daily_return_std * np.sqrt(trading_days_per_year)\n",
    "        sharpe_ratio = annualized_return / annualized_vol if annualized_vol > 0 else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics.append({\n",
    "            'Regime': regime,\n",
    "            'Count': len(regime_data),\n",
    "            'Avg_Daily_Return': daily_return_mean,\n",
    "            'Daily_Std': daily_return_std,\n",
    "            'Hit_Rate': hit_rate,\n",
    "            'Max_Drawdown': max_drawdown,\n",
    "            'Annualized_Return': annualized_return,\n",
    "            'Annualized_Vol': annualized_vol,\n",
    "            'Sharpe_Ratio': sharpe_ratio\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# ======== VISUALIZATION FUNCTIONS ========\n",
    "def plot_regimes(results, title=None):\n",
    "    \"\"\"Plot SPY price with regime classifications\"\"\"\n",
    "    if results is None or len(results) == 0:\n",
    "        print(\"No data available to plot\")\n",
    "        return\n",
    "    \n",
    "    # Set plot title\n",
    "    if title is None:\n",
    "        start_date = results['Date'].min().strftime('%Y-%m-%d')\n",
    "        end_date = results['Date'].max().strftime('%Y-%m-%d')\n",
    "        title = f'Market Regimes from {start_date} to {end_date}'\n",
    "    \n",
    "    # Define specific colors for each regime type\n",
    "    color_map = {\n",
    "        'Bull': 'green',\n",
    "        'Bear': 'red',\n",
    "        'Neutral': 'gold'\n",
    "    }\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig = make_subplots(rows=3, cols=1, \n",
    "                        shared_xaxes=True,\n",
    "                        vertical_spacing=0.1,\n",
    "                        row_heights=[0.5, 0.25, 0.25],\n",
    "                        subplot_titles=(title, \"Regime Probabilities\", \"Regime Performance\"))\n",
    "    \n",
    "    # Add price line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=results['Date'],\n",
    "            y=results['Close'],\n",
    "            mode='lines',\n",
    "            line=dict(color='rgba(0,0,0,0.3)', width=1),\n",
    "            name=f'{TICKER} Price'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add colored markers for different regimes\n",
    "    unique_regimes = results['Regime_Label'].unique()\n",
    "    for regime in sorted(unique_regimes):\n",
    "        regime_data = results[results['Regime_Label'] == regime]\n",
    "        \n",
    "        # Use our predefined colors based on regime label\n",
    "        color = color_map.get(regime, 'gray')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=regime_data['Date'], \n",
    "                y=regime_data['Close'],\n",
    "                mode='markers',\n",
    "                marker=dict(color=color, size=6),\n",
    "                name=f'{regime} Regime',\n",
    "                hovertemplate='%{x}<br>Price: %{y:.2f}<br>Regime: ' + regime + \n",
    "                              '<br>Probability: %{text:.2f}',\n",
    "                text=regime_data['Regime_Probability']\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Add regime probability traces in the second subplot\n",
    "    # Use smoothed probabilities if available\n",
    "    prob_prefix = 'Smooth_Prob_' if 'Smooth_Prob_Bull' in results.columns else 'Prob_'\n",
    "    \n",
    "    for regime in sorted(unique_regimes):\n",
    "        color = color_map.get(regime, 'gray')\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=results['Date'],\n",
    "                y=results[f'{prob_prefix}{regime}'],\n",
    "                mode='lines',\n",
    "                line=dict(width=2, color=color),\n",
    "                name=f'{regime} Probability'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Add a horizontal line at 0.5 probability\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=results['Date'].min(),\n",
    "        y0=0.5,\n",
    "        x1=results['Date'].max(),\n",
    "        y1=0.5,\n",
    "        line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Calculate cumulative returns by regime\n",
    "    results['Return'] = results['Log_Return'] / 100  # Convert to decimal\n",
    "    \n",
    "    # Calculate regime-specific returns\n",
    "    for regime in sorted(unique_regimes):\n",
    "        mask = results['Regime_Label'] == regime\n",
    "        results[f'Return_{regime}'] = results['Return'].copy()\n",
    "        results.loc[~mask, f'Return_{regime}'] = 0\n",
    "        \n",
    "        # Calculate cumulative returns (1 + r)\n",
    "        results[f'Cumulative_{regime}'] = (1 + results[f'Return_{regime}']).cumprod()\n",
    "        \n",
    "        # Add to the third subplot\n",
    "        color = color_map.get(regime, 'gray')\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=results['Date'],\n",
    "                y=results[f'Cumulative_{regime}'],\n",
    "                mode='lines',\n",
    "                line=dict(width=2, color=color),\n",
    "                name=f'{regime} Performance'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # Also add the overall buy-and-hold performance\n",
    "    results['Cumulative_All'] = (1 + results['Return']).cumprod()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=results['Date'],\n",
    "            y=results['Cumulative_All'],\n",
    "            mode='lines',\n",
    "            line=dict(width=2, color='black', dash='dash'),\n",
    "            name='Buy & Hold'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title=f'{TICKER} Price',\n",
    "        yaxis2_title='Probability',\n",
    "        yaxis3_title='Growth of $1',\n",
    "        template='plotly_white',\n",
    "        legend_title='Market Regimes',\n",
    "        hovermode='closest',\n",
    "        height=1000\n",
    "    )\n",
    "    \n",
    "    # Set y-axis range for probability subplot\n",
    "    fig.update_yaxes(range=[0, 1], row=2, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create pie chart showing regime distribution\n",
    "    regime_distribution = results['Regime_Label'].value_counts().reset_index()\n",
    "    regime_distribution.columns = ['Regime', 'Days']\n",
    "    regime_distribution['Percentage'] = regime_distribution['Days'] / len(results) * 100\n",
    "    \n",
    "    # Calculate performance metrics by regime\n",
    "    metrics = calculate_conditional_metrics(results)\n",
    "    \n",
    "    # Print regime performance metrics\n",
    "    print(\"\\nRegime Performance Metrics:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Regime':<10} {'Days':<10} {'Annualized Return':<20} {'Annualized Vol':<20} {'Sharpe':<10} {'Hit Rate':<10} {'Max DD':<10}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for _, row in metrics.iterrows():\n",
    "        print(f\"{row['Regime']:<10} {row['Count']:<10.0f} {row['Annualized_Return']:<20.2f} \"\n",
    "              f\"{row['Annualized_Vol']:<20.2f} {row['Sharpe_Ratio']:<10.2f} {row['Hit_Rate']:<10.2f} {row['Max_Drawdown']:<10.2f}\")\n",
    "    \n",
    "    # Create performance heatmap\n",
    "    fig_perf = px.bar(\n",
    "        metrics, \n",
    "        x='Regime', \n",
    "        y='Annualized_Return',\n",
    "        color='Regime',\n",
    "        color_discrete_map=color_map,\n",
    "        title='Annualized Returns by Regime',\n",
    "        text='Sharpe_Ratio',\n",
    "        labels={'Annualized_Return': 'Annualized Return (%)', 'Regime': 'Market Regime'}\n",
    "    )\n",
    "    \n",
    "    fig_perf.update_layout(\n",
    "        yaxis_title='Annualized Return (%)',\n",
    "        xaxis_title='Market Regime',\n",
    "        template='plotly_white',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_perf.show()\n",
    "    \n",
    "    # Create regime duration chart\n",
    "    regime_durations = []\n",
    "    current_regime = None\n",
    "    current_start = None\n",
    "    \n",
    "    # Calculate regime duration periods\n",
    "    for i, row in results.iterrows():\n",
    "        if current_regime is None:\n",
    "            current_regime = row['Regime_Label']\n",
    "            current_start = row['Date']\n",
    "        elif row['Regime_Label'] != current_regime:\n",
    "            # Regime change detected\n",
    "            end_date = row['Date']\n",
    "            duration = (end_date - current_start).days\n",
    "            \n",
    "            regime_durations.append({\n",
    "                'Regime': current_regime,\n",
    "                'Start': current_start,\n",
    "                'End': end_date,\n",
    "                'Duration': duration\n",
    "            })\n",
    "            \n",
    "            current_regime = row['Regime_Label']\n",
    "            current_start = row['Date']\n",
    "    \n",
    "    # Add the last regime period\n",
    "    if current_regime is not None:\n",
    "        end_date = results['Date'].iloc[-1]\n",
    "        duration = (end_date - current_start).days\n",
    "        \n",
    "        regime_durations.append({\n",
    "            'Regime': current_regime,\n",
    "            'Start': current_start,\n",
    "            'End': end_date,\n",
    "            'Duration': duration\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    regime_periods = pd.DataFrame(regime_durations)\n",
    "    \n",
    "    # Print regime periods\n",
    "    if len(regime_periods) > 0:\n",
    "        print(\"\\nRegime Periods:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Regime':<10} {'Start':<12} {'End':<12} {'Duration (days)':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, row in regime_periods.iterrows():\n",
    "            print(f\"{row['Regime']:<10} {row['Start'].strftime('%Y-%m-%d'):<12} {row['End'].strftime('%Y-%m-%d'):<12} {row['Duration']:<15.0f}\")\n",
    "    \n",
    "    return metrics, regime_periods\n",
    "\n",
    "def plot_viterbi_comparison(original_states, viterbi_states, dates, close_prices, regime_labels):\n",
    "    \"\"\"Compare original predictions with Viterbi algorithm smoothed predictions\"\"\"\n",
    "    \n",
    "    fig = make_subplots(rows=2, cols=1, \n",
    "                       shared_xaxes=True,\n",
    "                       vertical_spacing=0.1,\n",
    "                       row_heights=[0.7, 0.3],\n",
    "                       subplot_titles=('Original vs. Viterbi Smoothed Regimes', 'Regime Changes'))\n",
    "    \n",
    "    # Define colors for regimes\n",
    "    color_map = {\n",
    "        'Bull': 'green',\n",
    "        'Bear': 'red',\n",
    "        'Neutral': 'gold'\n",
    "    }\n",
    "    \n",
    "    # Add price line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=dates,\n",
    "            y=close_prices,\n",
    "            mode='lines',\n",
    "            line=dict(color='rgba(0,0,0,0.3)', width=1),\n",
    "            name='Price'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add original states\n",
    "    original_labels = [regime_labels[s] for s in original_states]\n",
    "    for regime in sorted(set(original_labels)):\n",
    "        mask = np.array(original_labels) == regime\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dates[mask],\n",
    "                y=close_prices[mask],\n",
    "                mode='markers',\n",
    "                marker=dict(color=color_map.get(regime, 'gray'), size=8, symbol='circle'),\n",
    "                name=f'Original {regime}'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Add viterbi states with different marker\n",
    "    viterbi_labels = [regime_labels[s] for s in viterbi_states]\n",
    "    for regime in sorted(set(viterbi_labels)):\n",
    "        mask = np.array(viterbi_labels) == regime\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dates[mask],\n",
    "                y=close_prices[mask],\n",
    "                mode='markers',\n",
    "                marker=dict(color=color_map.get(regime, 'gray'), size=4, symbol='x'),\n",
    "                name=f'Viterbi {regime}'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Add a trace showing where states differ\n",
    "    differ_mask = original_states != viterbi_states\n",
    "    \n",
    "    if np.any(differ_mask):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dates[differ_mask],\n",
    "                y=close_prices[differ_mask],\n",
    "                mode='markers',\n",
    "                marker=dict(color='purple', size=10, symbol='star'),\n",
    "                name='States Differ'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Add regime change indicators\n",
    "    original_changes = np.diff(original_states, prepend=original_states[0]) != 0\n",
    "    viterbi_changes = np.diff(viterbi_states, prepend=viterbi_states[0]) != 0\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=dates,\n",
    "            y=original_changes.astype(int),\n",
    "            mode='lines',\n",
    "            line=dict(color='blue', width=2),\n",
    "            name='Original Changes'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=dates,\n",
    "            y=viterbi_changes.astype(int),\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=2),\n",
    "            name='Viterbi Changes'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Comparison of Original vs. Viterbi Smoothed Regimes',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Price',\n",
    "        template='plotly_white',\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(title='Regime Change', range=[-0.1, 1.1], row=2, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Print statistics about the differences\n",
    "    total_differ = np.sum(differ_mask)\n",
    "    original_change_count = np.sum(original_changes)\n",
    "    viterbi_change_count = np.sum(viterbi_changes)\n",
    "    \n",
    "    print(f\"State Difference Statistics:\")\n",
    "    print(f\"Total days with different regimes: {total_differ} ({total_differ/len(dates)*100:.2f}%)\")\n",
    "    print(f\"Original regime changes: {original_change_count}\")\n",
    "    print(f\"Viterbi regime changes: {viterbi_change_count}\")\n",
    "    print(f\"Reduction in regime changes: {original_change_count - viterbi_change_count} ({(original_change_count - viterbi_change_count)/original_change_count*100:.2f}%)\")\n",
    "    \n",
    "    return total_differ, original_change_count, viterbi_change_count\n",
    "\n",
    "def plot_feature_importance(model_bundle, training_data):\n",
    "    \"\"\"\n",
    "    Plot feature importance for the HMM model based on component means\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        model = model_bundle['model']\n",
    "        selected_features = model_bundle['selected_features']\n",
    "        regime_labels = model_bundle['regime_labels']\n",
    "        pca = model_bundle['pca']\n",
    "        \n",
    "        # Get means from the model\n",
    "        means = model.means_\n",
    "        \n",
    "        # If we're using PCA, we need to transform means back to original feature space\n",
    "        n_components = pca.n_components_\n",
    "        \n",
    "        # Create a more interpretable feature importance visualization\n",
    "        # 1. Get PCA component loadings\n",
    "        component_loadings = pca.components_.T\n",
    "        \n",
    "        # Print the PCA loadings to help interpret components\n",
    "        loading_df = pd.DataFrame(\n",
    "            component_loadings,\n",
    "            index=selected_features,\n",
    "            columns=[f\"PC{i+1}\" for i in range(n_components)]\n",
    "        )\n",
    "        \n",
    "        print(\"PCA Component Loadings:\")\n",
    "        print(loading_df)\n",
    "        \n",
    "        # 2. Plot a heatmap of means in PCA space\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        sns.heatmap(means, cmap='RdBu_r', center=0,\n",
    "                   xticklabels=[f\"PC{i+1}\" for i in range(n_components)],\n",
    "                   yticklabels=[f\"Regime {i} ({regime_labels[i]})\" for i in range(len(regime_labels))],\n",
    "                   annot=True, fmt=\".2f\")\n",
    "        \n",
    "        plt.title('Regime Means in PCA Space')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Generate approximate feature importance by combining PCA loadings with means\n",
    "        feature_importance = {}\n",
    "        \n",
    "        for i, feature in enumerate(selected_features):\n",
    "            importance = 0\n",
    "            for j in range(n_components):\n",
    "                importance += abs(component_loadings[i, j]) * np.std(means[:, j])\n",
    "            feature_importance[feature] = importance\n",
    "        \n",
    "        # Sort and plot feature importance\n",
    "        sorted_importance = {k: v for k, v in sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)}\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.bar(sorted_importance.keys(), sorted_importance.values())\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title('Feature Importance (PCA-derived)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 4. Plot the state space for the first two principal components\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Extract feature data\n",
    "        feature_data = training_data[selected_features].values\n",
    "        scaler = model_bundle['scaler']\n",
    "        scaled_features = scaler.transform(feature_data)\n",
    "        \n",
    "        # Reduce to first two PCA components\n",
    "        reduced_features = pca.transform(scaled_features)\n",
    "        \n",
    "        # Get regime predictions for each data point\n",
    "        hidden_states = model.predict(reduced_features)\n",
    "        \n",
    "        # Plot data points colored by predicted regime\n",
    "        for i, label in enumerate(regime_labels):\n",
    "            mask = hidden_states == i\n",
    "            plt.scatter(\n",
    "                reduced_features[mask, 0],\n",
    "                reduced_features[mask, 1],\n",
    "                alpha=0.5,\n",
    "                label=label,\n",
    "                color=color_map.get(label, 'gray')\n",
    "            )\n",
    "        \n",
    "        # Plot regime centers\n",
    "        plt.scatter(\n",
    "            means[:, 0],\n",
    "            means[:, 1],\n",
    "            marker='*',\n",
    "            s=300,\n",
    "            edgecolor='k',\n",
    "            label='Regime Centers',\n",
    "            c=[color_map.get(label, 'gray') for label in regime_labels]\n",
    "        )\n",
    "        \n",
    "        # Add arrows for the top features\n",
    "        top_features = list(sorted_importance.keys())[:5]  # Top 5 features\n",
    "        top_indices = [selected_features.index(f) for f in top_features]\n",
    "        \n",
    "        for i in top_indices:\n",
    "            plt.arrow(\n",
    "                0, 0,\n",
    "                component_loadings[i, 0] * 3,\n",
    "                component_loadings[i, 1] * 3,\n",
    "                head_width=0.1,\n",
    "                head_length=0.1,\n",
    "                fc='black',\n",
    "                ec='black'\n",
    "            )\n",
    "            plt.text(\n",
    "                component_loadings[i, 0] * 3.1,\n",
    "                component_loadings[i, 1] * 3.1,\n",
    "                selected_features[i],\n",
    "                fontsize=12\n",
    "            )\n",
    "        \n",
    "        plt.xlabel(f'PC1 (Explains {pca.explained_variance_ratio_[0]:.1%} of variance)')\n",
    "        plt.ylabel(f'PC2 (Explains {pca.explained_variance_ratio_[1]:.1%} of variance)')\n",
    "        plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "        plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "        plt.title('HMM Regimes in PCA Space with Key Features')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Visualization requires matplotlib and seaborn. Install with: pip install matplotlib seaborn\")\n",
    "\n",
    "# ======== HYBRID APPROACH FUNCTIONS ========\n",
    "def train_hybrid_hmm_gmm(data, start_date, end_date, n_components=3):\n",
    "    \"\"\"\n",
    "    Hybrid approach combining GMM for initial clustering and HMM for temporal dynamics\n",
    "    \n",
    "    This two-stage approach often performs better than either model alone\n",
    "    \"\"\"\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "    # Filter data to training period\n",
    "    training = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)].copy()\n",
    "    \n",
    "    print(f\"Training hybrid GMM+HMM model on data from {start_date} to {end_date}\")\n",
    "    \n",
    "    # 1. Feature preparation - same as in HMM approach\n",
    "    reduced_features, scaler, pca, selected_features = select_and_prepare_features(\n",
    "        training, n_components=4\n",
    "    )\n",
    "    \n",
    "    # 2. First stage: GMM for initial clustering\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=n_components,\n",
    "        covariance_type='full',\n",
    "        max_iter=1000, \n",
    "        n_init=20,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit GMM\n",
    "    gmm.fit(reduced_features)\n",
    "    \n",
    "    # Get cluster assignments and probabilities\n",
    "    gmm_labels = gmm.predict(reduced_features)\n",
    "    gmm_probs = gmm.predict_proba(reduced_features)\n",
    "    \n",
    "    # Analyze GMM cluster properties to establish mapping to regimes\n",
    "    regime_stats = {}\n",
    "    for i in range(n_components):\n",
    "        cluster_mask = (gmm_labels == i)\n",
    "        if np.sum(cluster_mask) > 0:\n",
    "            regime_stats[i] = {\n",
    "                'count': np.sum(cluster_mask),\n",
    "                'return_avg': np.mean(training.loc[cluster_mask, 'Log_Return']),\n",
    "                'volatility_avg': np.mean(training.loc[cluster_mask, 'Volatility']),\n",
    "                'vix_avg': np.mean(training.loc[cluster_mask, 'VIX']),\n",
    "                'momentum_avg': np.mean(training.loc[cluster_mask, 'Momentum'])\n",
    "            }\n",
    "    \n",
    "    # Assign regime labels based on average returns\n",
    "    return_avgs = [(i, regime_stats[i]['return_avg']) for i in regime_stats.keys()]\n",
    "    return_avgs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    gmm_mapping = {}\n",
    "    regime_labels = ['Bull', 'Neutral', 'Bear']\n",
    "    \n",
    "    for idx, (cluster, _) in enumerate(return_avgs):\n",
    "        if idx == 0:\n",
    "            gmm_mapping[cluster] = 0  # Bull\n",
    "        elif idx == len(return_avgs) - 1:\n",
    "            gmm_mapping[cluster] = 2  # Bear\n",
    "        else:\n",
    "            gmm_mapping[cluster] = 1  # Neutral\n",
    "    \n",
    "    # Use GMM results to initialize HMM\n",
    "    # 3. Initialize HMM with GMM results\n",
    "    # Convert GMM clusters to HMM-friendly mapping\n",
    "    mapped_labels = np.array([gmm_mapping[label] for label in gmm_labels])\n",
    "    \n",
    "    # Calculate empirical transition matrix from GMM labels\n",
    "    trans_mat = np.zeros((n_components, n_components))\n",
    "    \n",
    "    for i in range(1, len(mapped_labels)):\n",
    "        prev_state = mapped_labels[i-1]\n",
    "        curr_state = mapped_labels[i]\n",
    "        trans_mat[prev_state, curr_state] += 1\n",
    "    \n",
    "    # Convert to probabilities and ensure no zero probabilities\n",
    "    for i in range(n_components):\n",
    "        row_sum = trans_mat[i, :].sum()\n",
    "        if row_sum > 0:\n",
    "            trans_mat[i, :] = trans_mat[i, :] / row_sum\n",
    "        else:\n",
    "            # If no transitions observed, use reasonable defaults\n",
    "            trans_mat[i, :] = 0.1 / (n_components - 1)\n",
    "            trans_mat[i, i] = 0.9\n",
    "    \n",
    "    # Create HMM model\n",
    "    model = hmm.GaussianHMM(\n",
    "        n_components=n_components,\n",
    "        covariance_type='full',\n",
    "        n_iter=1000,\n",
    "        tol=1e-6,\n",
    "        init_params=\"\",  # Don't initialize params, we'll set them manually\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Set initial parameters from GMM results\n",
    "    model.startprob_ = np.array([np.mean(mapped_labels == i) for i in range(n_components)])\n",
    "    model.transmat_ = trans_mat\n",
    "    \n",
    "    # Use GMM means and covariances for HMM initialization\n",
    "    # Reorder to match our mapping\n",
    "    n_features = reduced_features.shape[1]\n",
    "    model.means_ = np.zeros((n_components, n_features))\n",
    "    \n",
    "    # Initialize covariances properly according to the covariance type\n",
    "    if model.covariance_type == 'full':\n",
    "        # For 'full' covariance type, we need to set _covars_ directly\n",
    "        # with the correct shape (n_components, n_features, n_features)\n",
    "        covars = np.stack([np.eye(n_features) for _ in range(n_components)])\n",
    "        model._covars_ = covars\n",
    "    elif model.covariance_type == 'diag':\n",
    "        # For 'diag' covariance type, shape should be (n_components, n_features)\n",
    "        model._covars_ = np.ones((n_components, n_features))\n",
    "    elif model.covariance_type == 'tied':\n",
    "        # For 'tied' covariance type, shape should be (n_features, n_features)\n",
    "        model._covars_ = np.eye(n_features)\n",
    "    elif model.covariance_type == 'spherical':\n",
    "        # For 'spherical' covariance type, shape should be (n_components,)\n",
    "        model._covars_ = np.ones(n_components)\n",
    "    \n",
    "    # Map GMM means to HMM states\n",
    "    for gmm_idx, hmm_idx in gmm_mapping.items():\n",
    "        model.means_[hmm_idx] = gmm.means_[gmm_idx]\n",
    "        \n",
    "        # Carefully update covariances based on type\n",
    "        if model.covariance_type == 'full':\n",
    "            # Ensure covariance matrix is symmetric and positive-definite\n",
    "            cov = gmm.covariances_[gmm_idx]\n",
    "            # Make it symmetric\n",
    "            cov = (cov + cov.T) / 2\n",
    "            # Ensure positive-definiteness by adding small value to diagonal\n",
    "            cov += np.eye(cov.shape[0]) * 1e-3\n",
    "            model._covars_[hmm_idx] = cov\n",
    "        elif model.covariance_type == 'diag':\n",
    "            # Extract diagonal elements for diagonal covariance\n",
    "            model._covars_[hmm_idx] = np.diag(gmm.covariances_[gmm_idx])\n",
    "        elif model.covariance_type == 'tied' and hmm_idx == 0:  # Only once for tied\n",
    "            # Average of all covariances for tied covariance\n",
    "            cov = np.zeros_like(model._covars_)\n",
    "            for i in range(n_components):\n",
    "                cov += gmm.covariances_[i]\n",
    "            cov /= n_components\n",
    "            # Make it symmetric\n",
    "            cov = (cov + cov.T) / 2\n",
    "            # Ensure positive-definiteness\n",
    "            cov += np.eye(cov.shape[0]) * 1e-3\n",
    "            model._covars_ = cov\n",
    "        elif model.covariance_type == 'spherical':\n",
    "            # Use average of diagonal for spherical\n",
    "            model._covars_[hmm_idx] = np.mean(np.diag(gmm.covariances_[gmm_idx]))\n",
    "    \n",
    "    # 4. Train HMM with good initialization\n",
    "    model.fit(reduced_features)\n",
    "    \n",
    "    # Get state sequence and probabilities\n",
    "    hidden_states = model.predict(reduced_features)\n",
    "    state_probs = model.predict_proba(reduced_features)\n",
    "    \n",
    "    # Extract smoothed probabilities using Forward-Backward\n",
    "    _, forwback = model.score_samples(reduced_features)\n",
    "    \n",
    "    # Add results to the training data\n",
    "    training['Predicted_Regime'] = hidden_states\n",
    "    training['Regime_Label'] = [regime_labels[s] for s in hidden_states]\n",
    "    training['Regime_Probability'] = np.max(state_probs, axis=1)\n",
    "    \n",
    "    # Add probability columns\n",
    "    for i, label in enumerate(regime_labels):\n",
    "        training[f'Prob_{label}'] = state_probs[:, i]\n",
    "        training[f'Smooth_Prob_{label}'] = forwback[:, i]\n",
    "    \n",
    "    # Add GMM results for comparison\n",
    "    training['GMM_Cluster'] = gmm_labels\n",
    "    training['GMM_Mapping'] = [gmm_mapping[c] for c in gmm_labels]\n",
    "    training['GMM_Label'] = [regime_labels[gmm_mapping[c]] for c in gmm_labels]\n",
    "    \n",
    "    # Calculate regime statistics\n",
    "    regime_stats = {}\n",
    "    for i in range(n_components):\n",
    "        regime_mask = (hidden_states == i)\n",
    "        if np.sum(regime_mask) > 0:\n",
    "            regime_stats[i] = {\n",
    "                'count': np.sum(regime_mask),\n",
    "                'return_avg': np.mean(training.loc[regime_mask, 'Log_Return']),\n",
    "                'return_std': np.std(training.loc[regime_mask, 'Log_Return']),\n",
    "                'volatility_avg': np.mean(training.loc[regime_mask, 'Volatility']),\n",
    "                'vix_avg': np.mean(training.loc[regime_mask, 'VIX']),\n",
    "                'vix_ratio_avg': np.mean(training.loc[regime_mask, 'VIX_Ratio']),\n",
    "                'momentum_avg': np.mean(training.loc[regime_mask, 'Momentum']),\n",
    "                'prob_avg': np.mean(state_probs[:, i])\n",
    "            }\n",
    "    \n",
    "    # Calculate model evaluation metrics\n",
    "    # Calculate number of parameters for the HMM\n",
    "    n_states = model.n_components\n",
    "    n_features = model.means_.shape[1]\n",
    "    \n",
    "    # Number of parameters depends on covariance type\n",
    "    if model.covariance_type == 'full':\n",
    "        n_cov_params = n_states * n_features * (n_features + 1) // 2\n",
    "    elif model.covariance_type == 'diag':\n",
    "        n_cov_params = n_states * n_features\n",
    "    elif model.covariance_type == 'tied':\n",
    "        n_cov_params = n_features * (n_features + 1) // 2\n",
    "    elif model.covariance_type == 'spherical':\n",
    "        n_cov_params = n_states\n",
    "    \n",
    "    # Calculate total number of parameters\n",
    "    n_params = n_states - 1  # startprob_\n",
    "    n_params += n_states * (n_states - 1)  # transmat_\n",
    "    n_params += n_states * n_features  # means_\n",
    "    n_params += n_cov_params  # covars_\n",
    "    \n",
    "    # Create model bundle\n",
    "    model_bundle = {\n",
    "        'model': model,\n",
    "        'gmm': gmm,\n",
    "        'gmm_mapping': gmm_mapping,\n",
    "        'scaler': scaler,\n",
    "        'pca': pca,\n",
    "        'selected_features': selected_features,\n",
    "        'regime_labels': regime_labels,\n",
    "        'n_components': n_components\n",
    "    }\n",
    "    \n",
    "    # Print key regime statistics\n",
    "    print(\"\\nHybrid Model - Regime Characteristics:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Regime':<8} {'Label':<8} {'Count':<8} {'Return %':<10} {'Vol':<8} {'VIX':<8} {'Momentum':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        if i in regime_stats:\n",
    "            stats = regime_stats[i]\n",
    "            print(f\"{i:<8} {regime_labels[i]:<8} {stats['count']:<8} \"\n",
    "                  f\"{stats['return_avg']:<10.2f} {stats['volatility_avg']:<8.2f} {stats['vix_avg']:<8.2f} \"\n",
    "                  f\"{stats['momentum_avg']:<10.2f}\")\n",
    "    \n",
    "    # Print transition matrix\n",
    "    print(\"\\nRegime Transition Matrix:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'From/To':<10}\", end=\"\")\n",
    "    for i in range(n_components):\n",
    "        print(f\"{regime_labels[i]:<10}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        print(f\"{regime_labels[i]:<10}\", end=\"\")\n",
    "        for j in range(n_components):\n",
    "            print(f\"{model.transmat_[i, j]:<10.4f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Compare GMM and HMM results\n",
    "    agreement = np.mean(training['GMM_Mapping'] == training['Predicted_Regime']) * 100\n",
    "    print(f\"\\nGMM and HMM agreement: {agreement:.2f}%\")\n",
    "    \n",
    "    # Calculate average regime duration\n",
    "    regime_changes = (training['Predicted_Regime'] != training['Predicted_Regime'].shift(1)).sum()\n",
    "    avg_duration = len(training) / (regime_changes if regime_changes > 0 else 1)\n",
    "    print(f\"HMM average regime duration: {avg_duration:.2f} days\")\n",
    "    \n",
    "    gmm_changes = (training['GMM_Mapping'] != training['GMM_Mapping'].shift(1)).sum()\n",
    "    gmm_avg_duration = len(training) / (gmm_changes if gmm_changes > 0 else 1)\n",
    "    print(f\"GMM average regime duration: {gmm_avg_duration:.2f} days\")\n",
    "    \n",
    "    return model_bundle, training, hidden_states, regime_labels, regime_stats\n",
    "\n",
    "# ======== MAIN EXECUTION ========\n",
    "def main():\n",
    "    \"\"\"Main execution function with model comparison\"\"\"\n",
    "    # Download and prepare market data\n",
    "    df = download_market_data(\n",
    "        TICKER, VIX_TICKER, TNX_TICKER, GLD_TICKER, \n",
    "        XLY_TICKER, XLP_TICKER, XLU_TICKER, XLF_TICKER,\n",
    "        HYG_TICKER, TLT_TICKER, START_DATE\n",
    "    )\n",
    "    \n",
    "    # Load and prepare A/D line data\n",
    "    ad_data = load_ad_line_data(\"nyse_breadth_2023.csv\")\n",
    "    \n",
    "    # Calculate features including A/D line indicators\n",
    "    df = calculate_features(df, ad_data)\n",
    "    \n",
    "    # 1. Train basic HMM model\n",
    "    hmm_model, hmm_training, hmm_states, regime_labels, _ = train_hmm_model(\n",
    "        df, TRAIN_START_DATE, TRAIN_END_DATE, NUM_REGIMES, MAX_ITERATIONS, COVARIANCE_TYPE\n",
    "    )\n",
    "    \n",
    "    # Plot training period results for HMM\n",
    "    print(\"\\nHMM Model - Training Period Results:\")\n",
    "    plot_regimes(hmm_training, f'HMM Market Regimes - Training Period ({TRAIN_START_DATE} to {TRAIN_END_DATE})')\n",
    "    \n",
    "    # 2. Train hybrid HMM+GMM model\n",
    "    hybrid_model, hybrid_training, hybrid_states, _, _ = train_hybrid_hmm_gmm(\n",
    "        df, TRAIN_START_DATE, TRAIN_END_DATE, NUM_REGIMES\n",
    "    )\n",
    "    \n",
    "    # Plot training period results for hybrid model\n",
    "    print(\"\\nHybrid Model - Training Period Results:\")\n",
    "    plot_regimes(hybrid_training, f'Hybrid GMM+HMM Market Regimes - Training Period ({TRAIN_START_DATE} to {TRAIN_END_DATE})')\n",
    "    \n",
    "    # 3. Test on recent period\n",
    "    recent_start = '2025-01-02'\n",
    "    recent_end = (datetime.today() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    #recent_end ='2025-03-03'\n",
    "    \n",
    "    # Predict with HMM\n",
    "    print(f\"\\nPredicting regimes for recent period ({recent_start} to {recent_end}) with HMM model:\")\n",
    "    hmm_recent = predict_regimes(hmm_model, df, recent_start, recent_end)\n",
    "    \n",
    "    # Predict with hybrid model\n",
    "    print(f\"\\nPredicting regimes for recent period ({recent_start} to {recent_end}) with hybrid model:\")\n",
    "    hybrid_recent = predict_regimes(hybrid_model, df, recent_start, recent_end)\n",
    "    \n",
    "    # Plot recent period results for both models\n",
    "    if hmm_recent is not None:\n",
    "        plot_regimes(hmm_recent, f'HMM Market Regimes - Recent Period ({recent_start} to {recent_end})')\n",
    "    \n",
    "    if hybrid_recent is not None:\n",
    "        plot_regimes(hybrid_recent, f'Hybrid GMM+HMM Market Regimes - Recent Period ({recent_start} to {recent_end})')\n",
    "    \n",
    "    # 4. Compare different smoothing approaches for HMM\n",
    "    if hmm_recent is not None:\n",
    "        # Get original predictions\n",
    "        orig_states = hmm_recent['Predicted_Regime'].values\n",
    "        \n",
    "        # Apply Viterbi algorithm for global path optimization\n",
    "        reduced_features = hmm_model['pca'].transform(\n",
    "            hmm_model['scaler'].transform(hmm_recent[hmm_model['selected_features']].values)\n",
    "        )\n",
    "        viterbi_states = apply_viterbi_smoothing(hmm_model['model'], reduced_features)\n",
    "        \n",
    "        # Apply transition constraints for minimum duration\n",
    "        constrained_states = apply_transition_constraints(orig_states, min_duration=10)\n",
    "        \n",
    "        # Compare results\n",
    "        plot_viterbi_comparison(\n",
    "            orig_states, viterbi_states, \n",
    "            hmm_recent['Date'].values, hmm_recent['Close'].values,\n",
    "            hmm_model['regime_labels']\n",
    "        )\n",
    "        \n",
    "        # Compare duration-constrained to original\n",
    "        plot_viterbi_comparison(\n",
    "            orig_states, constrained_states,\n",
    "            hmm_recent['Date'].values, hmm_recent['Close'].values,\n",
    "            hmm_model['regime_labels']\n",
    "        )\n",
    "    \n",
    "    # 5. Create performance report and analysis\n",
    "    print(\"\\nModel Comparison Performance Report:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    if hmm_recent is not None and hybrid_recent is not None:\n",
    "        # Calculate performance metrics\n",
    "        hmm_metrics = calculate_conditional_metrics(hmm_recent)\n",
    "        hybrid_metrics = calculate_conditional_metrics(hybrid_recent)\n",
    "        \n",
    "        # Compare models on key metrics\n",
    "        print(\"\\nHMM Model Regime Performance:\")\n",
    "        for _, row in hmm_metrics.iterrows():\n",
    "            print(f\"{row['Regime']:<10} Annual Return: {row['Annualized_Return']:.2f}%, \"\n",
    "                  f\"Sharpe: {row['Sharpe_Ratio']:.2f}, Hit Rate: {row['Hit_Rate']:.2f}%\")\n",
    "        \n",
    "        print(\"\\nHybrid Model Regime Performance:\")\n",
    "        for _, row in hybrid_metrics.iterrows():\n",
    "            print(f\"{row['Regime']:<10} Annual Return: {row['Annualized_Return']:.2f}%, \"\n",
    "                  f\"Sharpe: {row['Sharpe_Ratio']:.2f}, Hit Rate: {row['Hit_Rate']:.2f}%\")\n",
    "        \n",
    "        # Calculate disagreement between models\n",
    "        merged = pd.merge(\n",
    "            hmm_recent[['Date', 'Regime_Label']], \n",
    "            hybrid_recent[['Date', 'Regime_Label']], \n",
    "            on='Date', suffixes=('_HMM', '_Hybrid')\n",
    "        )\n",
    "        \n",
    "        disagreement = (merged['Regime_Label_HMM'] != merged['Regime_Label_Hybrid']).mean() * 100\n",
    "        print(f\"\\nModel Disagreement: {disagreement:.2f}% of days\")\n",
    "        \n",
    "        # Create a confusion matrix between models\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        \n",
    "        labels = sorted(list(set(regime_labels)))\n",
    "        conf_matrix = confusion_matrix(\n",
    "            merged['Regime_Label_HMM'], \n",
    "            merged['Regime_Label_Hybrid'],\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        print(\"\\nConfusion Matrix (HMM vs Hybrid):\")\n",
    "        print(f\"{'HMM Hybrid':<15}\", end=\"\")\n",
    "        for label in labels:\n",
    "            print(f\"{label:<10}\", end=\"\")\n",
    "        print()\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"{label:<15}\", end=\"\")\n",
    "            for j in range(len(labels)):\n",
    "                print(f\"{conf_matrix[i, j]:<10}\", end=\"\")\n",
    "            print()\n",
    "    \n",
    "    # 6. Apply model to create example trading strategy\n",
    "    if hmm_recent is not None:\n",
    "        # Simple regime-based allocation\n",
    "        # Bull: 100% SPY\n",
    "        # Neutral: 50% SPY, 50% Cash\n",
    "        # Bear: 30% SPY, 70% Cash/Bond/Gold\n",
    "        \n",
    "        hmm_recent['Strategy_Allocation'] = hmm_recent['Regime_Label'].map({\n",
    "            'Bull': 1.0,    # 100% SPY\n",
    "            'Neutral': 0.5, # 50% SPY\n",
    "            'Bear': 0.3     # 30% SPY\n",
    "        })\n",
    "        \n",
    "        # Calculate strategy returns\n",
    "        hmm_recent['Strategy_Return'] = hmm_recent['Strategy_Allocation'] * hmm_recent['Log_Return'] / 100\n",
    "        \n",
    "        # Calculate cumulative returns\n",
    "        hmm_recent['SPY_Cumulative'] = (1 + hmm_recent['Log_Return'] / 100).cumprod()\n",
    "        hmm_recent['Strategy_Cumulative'] = (1 + hmm_recent['Strategy_Return']).cumprod()\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        spy_annual_return = np.mean(hmm_recent['Log_Return']) * 252  # Annualized\n",
    "        spy_annual_vol = np.std(hmm_recent['Log_Return']) * np.sqrt(252)\n",
    "        spy_sharpe = spy_annual_return / spy_annual_vol if spy_annual_vol > 0 else 0\n",
    "        \n",
    "        strat_annual_return = np.mean(hmm_recent['Strategy_Return'] * 100) * 252  # Annualized\n",
    "        strat_annual_vol = np.std(hmm_recent['Strategy_Return'] * 100) * np.sqrt(252)\n",
    "        strat_sharpe = strat_annual_return / strat_annual_vol if strat_annual_vol > 0 else 0\n",
    "        \n",
    "        # Plot strategy performance\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=hmm_recent['Date'],\n",
    "                y=hmm_recent['SPY_Cumulative'],\n",
    "                mode='lines',\n",
    "                name='SPY Buy & Hold',\n",
    "                line=dict(color='blue')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=hmm_recent['Date'],\n",
    "                y=hmm_recent['Strategy_Cumulative'],\n",
    "                mode='lines',\n",
    "                name='HMM Regime Strategy',\n",
    "                line=dict(color='green')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add regime background\n",
    "        for regime in sorted(hmm_recent['Regime_Label'].unique()):\n",
    "            regime_data = hmm_recent[hmm_recent['Regime_Label'] == regime]\n",
    "            if len(regime_data) > 0:\n",
    "                for i in range(len(regime_data) - 1):\n",
    "                    color = {'Bull': 'rgba(0,255,0,0.1)', 'Neutral': 'rgba(255,255,0,0.1)', 'Bear': 'rgba(255,0,0,0.1)'}\n",
    "                    \n",
    "                    fig.add_shape(\n",
    "                        type=\"rect\",\n",
    "                        x0=regime_data['Date'].iloc[i],\n",
    "                        y0=0,\n",
    "                        x1=regime_data['Date'].iloc[i+1],\n",
    "                        y1=max(hmm_recent['SPY_Cumulative'].max(), hmm_recent['Strategy_Cumulative'].max()) * 1.1,\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=color.get(regime, 'rgba(0,0,0,0.1)')\n",
    "                    )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'HMM Regime-Based Strategy Performance ({recent_start} to {recent_end})',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Growth of $1',\n",
    "            legend_title='Strategy',\n",
    "            template='plotly_white',\n",
    "            height=600,\n",
    "            annotations=[\n",
    "                dict(\n",
    "                    x=0.02, y=0.98, xref=\"paper\", yref=\"paper\",\n",
    "                    text=f\"SPY: {spy_annual_return:.2f}% Ann. Return, {spy_sharpe:.2f} Sharpe\",\n",
    "                    showarrow=False, align=\"left\", bgcolor=\"rgba(255,255,255,0.8)\"\n",
    "                ),\n",
    "                dict(\n",
    "                    x=0.02, y=0.93, xref=\"paper\", yref=\"paper\",\n",
    "                    text=f\"Strategy: {strat_annual_return:.2f}% Ann. Return, {strat_sharpe:.2f} Sharpe\",\n",
    "                    showarrow=False, align=\"left\", bgcolor=\"rgba(255,255,255,0.8)\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Print strategy performance\n",
    "        print(f\"\\nStrategy Performance ({recent_start} to {recent_end}):\")\n",
    "        print(f\"SPY Buy & Hold: {spy_annual_return:.2f}% annual return, {spy_annual_vol:.2f}% volatility, {spy_sharpe:.2f} Sharpe\")\n",
    "        print(f\"HMM Regime Strategy: {strat_annual_return:.2f}% annual return, {strat_annual_vol:.2f}% volatility, {strat_sharpe:.2f} Sharpe\")\n",
    "        \n",
    "        # Calculate max drawdowns\n",
    "        def calculate_max_drawdown(returns):\n",
    "            cumulative = (1 + returns).cumprod()\n",
    "            running_max = cumulative.cummax()\n",
    "            drawdown = (cumulative / running_max - 1) * 100\n",
    "            return drawdown.min()\n",
    "        \n",
    "        spy_drawdown = calculate_max_drawdown(hmm_recent['Log_Return'] / 100)\n",
    "        strat_drawdown = calculate_max_drawdown(hmm_recent['Strategy_Return'])\n",
    "        \n",
    "        print(f\"SPY Maximum Drawdown: {spy_drawdown:.2f}%\")\n",
    "        print(f\"Strategy Maximum Drawdown: {strat_drawdown:.2f}%\")\n",
    "    \n",
    "    # Return all models and data for further analysis\n",
    "    return {\n",
    "        'hmm_model': hmm_model,\n",
    "        'hybrid_model': hybrid_model,\n",
    "        'data': df,\n",
    "        'ad_data': ad_data,\n",
    "        'hmm_training': hmm_training,\n",
    "        'hybrid_training': hybrid_training,\n",
    "        'hmm_recent': hmm_recent if 'hmm_recent' in locals() else None,\n",
    "        'hybrid_recent': hybrid_recent if 'hybrid_recent' in locals() else None\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_objects = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trader_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
