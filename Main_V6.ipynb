{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Regimes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ======== CONFIGURABLE PARAMETERS ========\n",
    "# Market data parameters\n",
    "TICKER = 'SPY'  # Main ticker to analyze\n",
    "VIX_TICKER = '^VIX'  # Volatility index\n",
    "TNX_TICKER = '^TNX'  # 10-Year Treasury Yield\n",
    "START_DATE = \"1995-01-01\"  # Historical data start date\n",
    "\n",
    "# GMM model parameters  \n",
    "NUM_REGIMES = 3  # Number of market regimes\n",
    "MAX_ITERATIONS = 1000  # Max iterations for GMM convergence\n",
    "N_INIT = 100  # Number of initializations\n",
    "COVARIANCE_TYPE = 'diag'  # Options: 'full', 'tied', 'diag', 'spherical'\n",
    "\n",
    "# Technical indicator parameters\n",
    "VOL_WINDOW = 21  # Window for volatility calculation (21 days ~ 1 month)\n",
    "MOMENTUM_WINDOW = 63  # Window for momentum calculation (63 days ~ 3 months)\n",
    "SMA_FAST = 20  # Fast moving average\n",
    "SMA_SLOW = 50  # Slow moving average\n",
    "BB_WINDOW = 20  # Bollinger Bands window\n",
    "BB_STD = 2  # Bollinger Bands standard deviation multiplier\n",
    "ATR_WINDOW = 14  # Average True Range window\n",
    "CHOP_WINDOW = 14  # Choppiness Index window\n",
    "\n",
    "# Training period\n",
    "TRAIN_START_DATE = \"1995-01-01\"\n",
    "TRAIN_END_DATE = \"2024-01-01\"\n",
    "\n",
    "# ======== DATA PREPARATION FUNCTIONS ========\n",
    "def download_market_data(ticker, vix_ticker, tnx_ticker, start_date):\n",
    "    \"\"\"Download and prepare market data\"\"\"\n",
    "    end_date = (datetime.today() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Downloading market data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # Download ticker, VIX, and TNX data\n",
    "    df_ticker = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True)\n",
    "    df_vix = yf.download(vix_ticker, start=start_date, end=end_date)\n",
    "    df_tnx = yf.download(tnx_ticker, start=start_date, end=end_date)\n",
    "    \n",
    "    # Fix column structure and reset index\n",
    "    if len(df_ticker.columns.names) > 1:\n",
    "        df_ticker.columns = df_ticker.columns.droplevel(1)\n",
    "    if len(df_vix.columns.names) > 1:\n",
    "        df_vix.columns = df_vix.columns.droplevel(1)\n",
    "    if len(df_tnx.columns.names) > 1:\n",
    "        df_tnx.columns = df_tnx.columns.droplevel(1)\n",
    "    \n",
    "    df_ticker = df_ticker.reset_index()\n",
    "    df_vix = df_vix.reset_index()\n",
    "    df_tnx = df_tnx.reset_index()\n",
    "    \n",
    "    # Keep only Date and Close from VIX\n",
    "    df_vix = df_vix[['Date', 'Close']].rename(columns={'Close': 'VIX'})\n",
    "    \n",
    "    # Keep only Date and Close from TNX\n",
    "    df_tnx = df_tnx[['Date', 'Close']].rename(columns={'Close': 'TNX'})\n",
    "    \n",
    "    # Merge data\n",
    "    df = pd.merge(df_ticker, df_vix, on='Date', how='left')\n",
    "    df = pd.merge(df, df_tnx, on='Date', how='left')\n",
    "    \n",
    "    # Fill missing values with forward fill method\n",
    "    df['VIX'] = df['VIX'].fillna(method='ffill')\n",
    "    df['TNX'] = df['TNX'].fillna(method='ffill')\n",
    "    \n",
    "    df['LogVIX'] = np.log(df['VIX'])\n",
    "    \n",
    "    # Calculate daily log returns\n",
    "    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1)) * 100\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_bollinger_bands(df, window=20, num_std=2):\n",
    "    \"\"\"Calculate Bollinger Bands and related metrics\"\"\"\n",
    "    # Calculate Bollinger Bands\n",
    "    df['BB_Middle'] = df['Close'].rolling(window=window).mean()\n",
    "    rolling_std = df['Close'].rolling(window=window).std()\n",
    "    df['BB_Upper'] = df['BB_Middle'] + (rolling_std * num_std)\n",
    "    df['BB_Lower'] = df['BB_Middle'] - (rolling_std * num_std)\n",
    "    \n",
    "    # Calculate Bollinger Band Width (normalized)\n",
    "    df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle'] * 100\n",
    "    \n",
    "    # Calculate %B (position within the bands)\n",
    "    df['BB_PercentB'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_atr(df, window=14):\n",
    "    \"\"\"Calculate Average True Range (ATR)\"\"\"\n",
    "    # Calculate True Range\n",
    "    df['TR1'] = abs(df['High'] - df['Low'])\n",
    "    df['TR2'] = abs(df['High'] - df['Close'].shift(1))\n",
    "    df['TR3'] = abs(df['Low'] - df['Close'].shift(1))\n",
    "    df['True_Range'] = df[['TR1', 'TR2', 'TR3']].max(axis=1)\n",
    "    \n",
    "    # Calculate ATR using Wilder's smoothing method\n",
    "    df['ATR'] = df['True_Range'].rolling(window=window).mean()\n",
    "    \n",
    "    # Normalize ATR by price\n",
    "    df['ATR_Normalized'] = df['ATR'] / df['Close'] * 100\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    df = df.drop(['TR1', 'TR2', 'TR3'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_choppiness_index(df, window=14):\n",
    "    \"\"\"\n",
    "    Calculate Choppiness Index\n",
    "    \n",
    "    CI = 100 * LOG10(SUM(ATR, n) / (MaxHi - MinLo)) / LOG10(n)\n",
    "    Where:\n",
    "    - n is the window period\n",
    "    - MaxHi is the highest high in the period\n",
    "    - MinLo is the lowest low in the period\n",
    "    - SUM(ATR, n) is the sum of the ATR over the period\n",
    "    \n",
    "    Higher values (> 61.8) indicate a choppy market\n",
    "    Lower values (< 38.2) indicate a trending market\n",
    "    \"\"\"\n",
    "    if 'ATR' not in df.columns:\n",
    "        df = calculate_atr(df, window)\n",
    "    \n",
    "    df['MaxHi'] = df['High'].rolling(window=window).max()\n",
    "    df['MinLo'] = df['Low'].rolling(window=window).min()\n",
    "    df['ATR_Sum'] = df['ATR'].rolling(window=window).sum()\n",
    "    \n",
    "    # Calculate Choppiness Index\n",
    "    df['Choppiness_Index'] = 100 * np.log10(df['ATR_Sum'] / (df['MaxHi'] - df['MinLo'])) / np.log10(window)\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    df = df.drop(['MaxHi', 'MinLo', 'ATR_Sum'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_features(data):\n",
    "    \"\"\"Calculate features for regime classification\"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Volatility features\n",
    "    df['Volatility'] = df['Log_Return'].rolling(window=VOL_WINDOW).std() * np.sqrt(252)  # Annualized\n",
    "    df['Volume_Z_Score'] = (df['Volume'] - df['Volume'].rolling(window=VOL_WINDOW).mean()) / df['Volume'].rolling(window=VOL_WINDOW).std()\n",
    "    df['VIX_Z_Score'] = (df['VIX'] - df['VIX'].rolling(window=VOL_WINDOW).mean()) / df['VIX'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    # Trend features\n",
    "    df['Momentum'] = df['Close'].pct_change(periods=MOMENTUM_WINDOW) * 100\n",
    "    df['SMA_Fast'] = df['Close'].rolling(window=SMA_FAST).mean()\n",
    "    df['SMA_Slow'] = df['Close'].rolling(window=SMA_SLOW).mean()\n",
    "    df['SMA_Ratio'] = df['SMA_Fast'] / df['SMA_Slow']\n",
    "    \n",
    "    # Price distance from moving averages\n",
    "    df['Price_to_SMA_Fast'] = df['Close'] / df['SMA_Fast'] - 1\n",
    "    df['Price_to_SMA_Slow'] = df['Close'] / df['SMA_Slow'] - 1\n",
    "    \n",
    "    # VIX-based features\n",
    "    df['VIX_Ratio'] = df['VIX'] / df['VIX'].rolling(window=VOL_WINDOW).mean()\n",
    "    df['VIX_Change'] = df['VIX'].pct_change(periods=5) * 100  # 5-day VIX change\n",
    "    \n",
    "    # Treasury Yield (TNX) features\n",
    "    df['TNX_Level'] = df['TNX']  # Absolute yield level\n",
    "    df['TNX_Daily_Change'] = df['TNX'].diff() * 100  # Daily change in basis points\n",
    "    df['TNX_Weekly_Change'] = df['TNX'].diff(5) * 100  # 1-week change in basis points\n",
    "    df['TNX_Z_Score'] = (df['TNX'] - df['TNX'].rolling(window=VOL_WINDOW).mean()) / df['TNX'].rolling(window=VOL_WINDOW).std()\n",
    "    df['TNX_Ratio'] = df['TNX'] / df['TNX'].rolling(window=VOL_WINDOW).mean()\n",
    "    \n",
    "    # Volume features\n",
    "    df['Volume_Trend'] = df['Volume'] / df['Volume'].rolling(window=VOL_WINDOW).mean()\n",
    "    df['Volume_to_Volatility'] = df['Volume'] / (df['Volatility'] + 1e-10)\n",
    "    \n",
    "    # Combined features\n",
    "    df['Return_Volatility_Ratio'] = df['Log_Return'] / (df['Volatility'] + 1e-10)\n",
    "    df['VIX_Volatility_Ratio'] = df['VIX'] / (df['Volatility'] + 1e-10)\n",
    "    \n",
    "    # Add Bollinger Bands features\n",
    "    df = calculate_bollinger_bands(df, window=BB_WINDOW, num_std=BB_STD)\n",
    "    \n",
    "    # Add ATR features\n",
    "    df = calculate_atr(df, window=ATR_WINDOW)\n",
    "    \n",
    "    # Add Choppiness Index\n",
    "    df = calculate_choppiness_index(df, window=CHOP_WINDOW)\n",
    "    \n",
    "    # Calculate Z-scores for the new indicators to help with comparison\n",
    "    df['BB_Width_Z'] = (df['BB_Width'] - df['BB_Width'].rolling(window=VOL_WINDOW).mean()) / df['BB_Width'].rolling(window=VOL_WINDOW).std()\n",
    "    df['ATR_Norm_Z'] = (df['ATR_Normalized'] - df['ATR_Normalized'].rolling(window=VOL_WINDOW).mean()) / df['ATR_Normalized'].rolling(window=VOL_WINDOW).std()\n",
    "    df['Choppiness_Z'] = (df['Choppiness_Index'] - df['Choppiness_Index'].rolling(window=VOL_WINDOW).mean()) / df['Choppiness_Index'].rolling(window=VOL_WINDOW).std()\n",
    "    \n",
    "    # Fill NaN values \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ======== GMM MODEL FUNCTIONS ========\n",
    "def train_gmm_model(data, start_date, end_date, n_components=3, max_iter=500, n_init=10, cov_type='full'):\n",
    "    \"\"\"Train GMM model on selected features\"\"\"\n",
    "    # Filter data to training period\n",
    "    training = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)].copy()\n",
    "    \n",
    "    print(f\"Training GMM model on data from {start_date} to {end_date}\")\n",
    "    print(f\"Training data shape: {training.shape}\")\n",
    "    \n",
    "    # Select features for GMM\n",
    "    features = [\n",
    "        'Log_Return',             # Daily returns\n",
    "        'Volatility',             # Recent volatility\n",
    "        'VIX_Z_Score',            # VIX relative to its recent history\n",
    "        'Price_to_SMA_Fast',      # Short-term trend\n",
    "        'Price_to_SMA_Slow',      # Long-term trend\n",
    "        'Momentum',               # Medium-term price momentum\n",
    "        'VIX_Ratio',              # VIX relative to its recent average\n",
    "        'Volume_Z_Score',         # Volume relative to its recent history\n",
    "        'BB_Width_Z',             # Normalized Bollinger Band width (consolidation)\n",
    "        'ATR_Norm_Z',             # Normalized ATR (relative volatility)\n",
    "        'Choppiness_Z',           # Market choppiness/trendiness\n",
    "        'TNX_Z_Score',            # 10-Year yield relative to recent history\n",
    "        'TNX_Weekly_Change'       # Weekly change in yields\n",
    "    ]\n",
    "    \n",
    "    feature_data = training[features].values\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = GaussianMixture(\n",
    "        n_components=n_components, \n",
    "        covariance_type=cov_type,\n",
    "        max_iter=max_iter,\n",
    "        n_init=n_init,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(scaled_features)\n",
    "    \n",
    "    # Get regime predictions and probabilities for training data\n",
    "    predictions = model.predict(scaled_features)\n",
    "    probabilities = model.predict_proba(scaled_features)\n",
    "    \n",
    "    # Analyze regime characteristics\n",
    "    regime_stats = {}\n",
    "    for i in range(n_components):\n",
    "        regime_mask = (predictions == i)\n",
    "        if np.sum(regime_mask) > 0:\n",
    "            regime_stats[i] = {\n",
    "                'count': np.sum(regime_mask),\n",
    "                'return_avg': np.mean(training.loc[regime_mask, 'Log_Return']),\n",
    "                'return_std': np.std(training.loc[regime_mask, 'Log_Return']),\n",
    "                'volatility_avg': np.mean(training.loc[regime_mask, 'Volatility']),\n",
    "                'vix_avg': np.mean(training.loc[regime_mask, 'VIX']),\n",
    "                'vix_ratio_avg': np.mean(training.loc[regime_mask, 'VIX_Ratio']),\n",
    "                'momentum_avg': np.mean(training.loc[regime_mask, 'Momentum']),\n",
    "                'volume_zscore_avg': np.mean(training.loc[regime_mask, 'Volume_Z_Score']),\n",
    "                'bb_width_avg': np.mean(training.loc[regime_mask, 'BB_Width']),\n",
    "                'atr_norm_avg': np.mean(training.loc[regime_mask, 'ATR_Normalized']),\n",
    "                'choppiness_avg': np.mean(training.loc[regime_mask, 'Choppiness_Index']),\n",
    "                'tnx_avg': np.mean(training.loc[regime_mask, 'TNX']),\n",
    "                'tnx_zscore_avg': np.mean(training.loc[regime_mask, 'TNX_Z_Score']),\n",
    "                'tnx_weekly_change_avg': np.mean(training.loc[regime_mask, 'TNX_Weekly_Change']),\n",
    "                'prob_avg': np.mean(probabilities[:, i])\n",
    "            }\n",
    "    \n",
    "    # Assign labels to regimes based on characteristics\n",
    "    regime_labels = [\"\"] * n_components\n",
    "    bull_scores = []\n",
    "    bear_scores = []\n",
    "    neutral_scores = []\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        if i not in regime_stats:\n",
    "            bull_scores.append(0)\n",
    "            bear_scores.append(0)\n",
    "            neutral_scores.append(0)\n",
    "            continue\n",
    "            \n",
    "        stats = regime_stats[i]\n",
    "        \n",
    "        # Bull regime scoring: high returns, positive momentum, lower VIX\n",
    "        bull_score = 0\n",
    "        if stats['return_avg'] > 0.05:\n",
    "            bull_score += 2\n",
    "        elif stats['return_avg'] > 0:\n",
    "            bull_score += 1\n",
    "        \n",
    "        if stats['momentum_avg'] > 5:\n",
    "            bull_score += 2\n",
    "        elif stats['momentum_avg'] > 0:\n",
    "            bull_score += 1\n",
    "            \n",
    "        if stats['vix_ratio_avg'] < 0.9:\n",
    "            bull_score += 2\n",
    "        elif stats['vix_ratio_avg'] < 1:\n",
    "            bull_score += 1\n",
    "            \n",
    "        # Add trend strength scoring (lower values indicate strong trend)\n",
    "        if stats['choppiness_avg'] < 38.2:\n",
    "            bull_score += 1\n",
    "        \n",
    "        # Bear regime scoring: negative returns, negative momentum, higher VIX\n",
    "        bear_score = 0\n",
    "        if stats['return_avg'] < -0.1:\n",
    "            bear_score += 2\n",
    "        elif stats['return_avg'] < 0:\n",
    "            bear_score += 1\n",
    "        \n",
    "        if stats['momentum_avg'] < -5:\n",
    "            bear_score += 2\n",
    "        elif stats['momentum_avg'] < 0:\n",
    "            bear_score += 1\n",
    "            \n",
    "        if stats['vix_ratio_avg'] > 1.2:\n",
    "            bear_score += 2\n",
    "        elif stats['vix_ratio_avg'] > 1:\n",
    "            bear_score += 1\n",
    "            \n",
    "        # Higher ATR often indicates bear markets (more volatile)\n",
    "        if stats['atr_norm_avg'] > np.mean([s['atr_norm_avg'] for s in regime_stats.values()]) * 1.2:\n",
    "            bear_score += 1\n",
    "            \n",
    "        # Add trend strength scoring (lower values indicate strong trend)\n",
    "        if stats['choppiness_avg'] < 38.2:\n",
    "            bear_score += 1\n",
    "            \n",
    "        # Add Treasury yield scoring (rising yields often coincide with bear markets)\n",
    "        if stats['tnx_weekly_change_avg'] > 10:  # 10 basis points per week is significant\n",
    "            bear_score += 1\n",
    "        \n",
    "        # Neutral regime scoring: modest returns, low volatility, normal VIX, choppy markets\n",
    "        neutral_score = 0\n",
    "        if abs(stats['return_avg']) < 0.05:\n",
    "            neutral_score += 2\n",
    "        elif abs(stats['return_avg']) < 0.1:\n",
    "            neutral_score += 1\n",
    "        \n",
    "        if abs(stats['momentum_avg']) < 3:\n",
    "            neutral_score += 2\n",
    "        elif abs(stats['momentum_avg']) < 5:\n",
    "            neutral_score += 1\n",
    "            \n",
    "        if 0.95 <= stats['vix_ratio_avg'] <= 1.05:\n",
    "            neutral_score += 2\n",
    "        elif 0.9 <= stats['vix_ratio_avg'] <= 1.1:\n",
    "            neutral_score += 1\n",
    "            \n",
    "        # Narrow Bollinger Bands often indicate consolidation (neutral)\n",
    "        if stats['bb_width_avg'] < np.mean([s['bb_width_avg'] for s in regime_stats.values()]) * 0.8:\n",
    "            neutral_score += 2\n",
    "            \n",
    "        # High Choppiness Index indicates consolidation (neutral)\n",
    "        if stats['choppiness_avg'] > 61.8:  # Traditional threshold\n",
    "            neutral_score += 2\n",
    "        elif stats['choppiness_avg'] > 50:\n",
    "            neutral_score += 1\n",
    "        \n",
    "        # Add Treasury yield scoring (stable yields often coincide with neutral markets)\n",
    "        if abs(stats['tnx_weekly_change_avg']) < 5:  # Small weekly changes\n",
    "            neutral_score += 1\n",
    "        \n",
    "        bull_scores.append(bull_score)\n",
    "        bear_scores.append(bear_score)\n",
    "        neutral_scores.append(neutral_score)\n",
    "    \n",
    "    # Assign labels based on highest score\n",
    "    labels_to_assign = [\"Bull\", \"Bear\", \"Neutral\"]\n",
    "    scores = [(i, max(bull_scores[i], bear_scores[i], neutral_scores[i]), \n",
    "               \"Bull\" if bull_scores[i] >= max(bear_scores[i], neutral_scores[i]) else\n",
    "               \"Bear\" if bear_scores[i] >= max(bull_scores[i], neutral_scores[i]) else\n",
    "               \"Neutral\") \n",
    "              for i in range(n_components)]\n",
    "    \n",
    "    # Sort by score and assign labels ensuring each label is used only once\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    assigned_labels = set()\n",
    "    \n",
    "    for regime_idx, _, preferred_label in scores:\n",
    "        if preferred_label not in assigned_labels:\n",
    "            regime_labels[regime_idx] = preferred_label\n",
    "            assigned_labels.add(preferred_label)\n",
    "        else:\n",
    "            # Find an unassigned label\n",
    "            for label in labels_to_assign:\n",
    "                if label not in assigned_labels:\n",
    "                    regime_labels[regime_idx] = label\n",
    "                    assigned_labels.add(label)\n",
    "                    break\n",
    "    \n",
    "    # Print key regime statistics\n",
    "    print(\"\\nRegime Characteristics Summary:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Regime':<8} {'Label':<8} {'Count':<8} {'Return %':<10} {'Vol':<8} {'VIX':<8} {'BB Width':<10} {'ATR %':<8} {'Chop Idx':<8} {'TNX %':<8} {'TNX Chg':<8}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        if i in regime_stats:\n",
    "            stats = regime_stats[i]\n",
    "            print(f\"{i:<8} {regime_labels[i]:<8} {stats['count']:<8} \"\n",
    "                  f\"{stats['return_avg']:<10.2f} {stats['volatility_avg']:<8.2f} {stats['vix_avg']:<8.2f} \"\n",
    "                  f\"{stats['bb_width_avg']:<10.2f} {stats['atr_norm_avg']:<8.2f} {stats['choppiness_avg']:<8.2f} \"\n",
    "                  f\"{stats['tnx_avg']:<8.2f} {stats['tnx_weekly_change_avg']:<8.2f}\")\n",
    "    \n",
    "    # Print scoring details\n",
    "    print(\"\\nRegime Scoring Details:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Regime':<8} {'Label':<8} {'Bull Score':<10} {'Bear Score':<10} {'Neutral Score':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        print(f\"{i:<8} {regime_labels[i]:<8} {bull_scores[i]:<10} {bear_scores[i]:<10} {neutral_scores[i]:<12}\")\n",
    "    \n",
    "    # Calculate BIC and AIC for model evaluation\n",
    "    print(f\"\\nModel Evaluation:\")\n",
    "    print(f\"BIC: {model.bic(scaled_features):.2f} (lower is better)\")\n",
    "    print(f\"AIC: {model.aic(scaled_features):.2f} (lower is better)\")\n",
    "    \n",
    "    # Calculate average regime duration and transitions\n",
    "    training['Regime'] = predictions\n",
    "    training['Regime_Probability'] = np.max(probabilities, axis=1)\n",
    "    \n",
    "    # Calculate regime transitions\n",
    "    regime_changes = (training['Regime'] != training['Regime'].shift(1)).sum()\n",
    "    avg_duration = len(training) / (regime_changes if regime_changes > 0 else 1)\n",
    "    print(f\"\\nRegime persistence: {avg_duration:.2f} days average duration\")\n",
    "    \n",
    "    # Calculate regime distribution\n",
    "    regime_counts = pd.Series(predictions).value_counts(normalize=True) * 100\n",
    "    print(\"\\nRegime Distribution:\")\n",
    "    for regime, percentage in sorted(regime_counts.items()):\n",
    "        print(f\"Regime {regime} [{regime_labels[regime]}]: {percentage:.2f}%\")\n",
    "    \n",
    "    # Find transition probability matrix\n",
    "    transitions = np.zeros((n_components, n_components))\n",
    "    for i in range(1, len(training)):\n",
    "        prev_regime = training['Regime'].iloc[i-1]\n",
    "        curr_regime = training['Regime'].iloc[i]\n",
    "        transitions[prev_regime, curr_regime] += 1\n",
    "    \n",
    "    # Convert transitions to percentages\n",
    "    for i in range(n_components):\n",
    "        row_sum = transitions[i, :].sum()\n",
    "        if row_sum > 0:\n",
    "            transitions[i, :] = transitions[i, :] / row_sum * 100\n",
    "    \n",
    "    # Print transition matrix\n",
    "    print(\"\\nRegime Transition Matrix:\")    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'From/To':<10}\", end=\"\")\n",
    "    for i in range(n_components):\n",
    "        print(f\"{regime_labels[i]:<10}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        print(f\"{regime_labels[i]:<10}\", end=\"\")\n",
    "        for j in range(n_components):\n",
    "            print(f\"{transitions[i, j]:<10.2f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Bundle model, scaler and other info\n",
    "    model_bundle = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'features': features,\n",
    "        'regime_labels': regime_labels\n",
    "    }\n",
    "    \n",
    "    return model_bundle, training, predictions, regime_labels, regime_stats\n",
    "\n",
    "def predict_regimes(model_bundle, data, start_date, end_date):\n",
    "    \"\"\"Predict market regimes for a specific date range using the trained GMM model\"\"\"\n",
    "    # Extract model components\n",
    "    model = model_bundle['model']\n",
    "    scaler = model_bundle['scaler']\n",
    "    features = model_bundle['features']\n",
    "    regime_labels = model_bundle['regime_labels']\n",
    "    \n",
    "    # Filter data for prediction period\n",
    "    pred_data = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)].copy()\n",
    "    \n",
    "    if len(pred_data) == 0:\n",
    "        print(f\"No data available for period {start_date} to {end_date}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    feature_data = pred_data[features].values\n",
    "    \n",
    "    # Scale features using the same scaler as training\n",
    "    scaled_features = scaler.transform(feature_data)\n",
    "    \n",
    "    # Predict regimes and probabilities\n",
    "    predictions = model.predict(scaled_features)\n",
    "    probabilities = model.predict_proba(scaled_features)\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    pred_data['Predicted_Regime'] = predictions\n",
    "    pred_data['Regime_Label'] = [regime_labels[r] for r in predictions]\n",
    "    pred_data['Regime_Probability'] = np.max(probabilities, axis=1)\n",
    "    \n",
    "    # Add probability columns for each regime\n",
    "    for i, label in enumerate(regime_labels):\n",
    "        pred_data[f'Prob_{label}'] = probabilities[:, i]\n",
    "    \n",
    "    # Print basic statistics about the prediction\n",
    "    print(f\"Predicted regimes for period {start_date} to {end_date}\")\n",
    "    \n",
    "    # Calculate regime distribution\n",
    "    regime_counts = pd.Series(predictions).value_counts(normalize=True) * 100\n",
    "    print(\"\\nRegime Distribution:\")\n",
    "    for regime, percentage in sorted(regime_counts.items()):\n",
    "        print(f\"Regime {regime} [{regime_labels[regime]}]: {percentage:.2f}%\")\n",
    "    \n",
    "    # Calculate average regime duration\n",
    "    regime_changes = (pred_data['Predicted_Regime'] != pred_data['Predicted_Regime'].shift(1)).sum()\n",
    "    avg_duration = len(pred_data) / (regime_changes if regime_changes > 0 else 1)\n",
    "    print(f\"\\nRegime persistence: {avg_duration:.2f} days average duration\")\n",
    "    \n",
    "    return pred_data\n",
    "\n",
    "# ======== VISUALIZATION FUNCTIONS ========\n",
    "def plot_regimes(results, title=None):\n",
    "    \"\"\"Plot SPY price with regime classifications\"\"\"\n",
    "    if results is None or len(results) == 0:\n",
    "        print(\"No data available to plot\")\n",
    "        return\n",
    "    \n",
    "    # Set plot title\n",
    "    if title is None:\n",
    "        start_date = results['Date'].min().strftime('%Y-%m-%d')\n",
    "        end_date = results['Date'].max().strftime('%Y-%m-%d')\n",
    "        title = f'Market Regimes from {start_date} to {end_date}'\n",
    "    \n",
    "    # Create a categorical color map for regimes\n",
    "    unique_regimes = results['Predicted_Regime'].unique()\n",
    "    n_regimes = len(unique_regimes)\n",
    "    \n",
    "    # Define specific colors for each regime type\n",
    "    color_map = {\n",
    "        'Bull': 'green',\n",
    "        'Bear': 'red',\n",
    "        'Neutral': 'gold'\n",
    "    }\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig = make_subplots(rows=2, cols=1, \n",
    "                        shared_xaxes=True,\n",
    "                        vertical_spacing=0.1,\n",
    "                        row_heights=[0.7, 0.3],\n",
    "                        subplot_titles=(title, \"Regime Probabilities\"))\n",
    "    \n",
    "    # Add price line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=results['Date'],\n",
    "            y=results['Close'],\n",
    "            mode='lines',\n",
    "            line=dict(color='rgba(0,0,0,0.3)', width=1),\n",
    "            name=f'{TICKER} Price'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add colored markers for different regimes\n",
    "    for i, regime in enumerate(sorted(unique_regimes)):\n",
    "        regime_data = results[results['Predicted_Regime'] == regime]\n",
    "        regime_label = results.loc[results['Predicted_Regime'] == regime, 'Regime_Label'].iloc[0]\n",
    "        \n",
    "        # Use our predefined colors based on regime label\n",
    "        color = color_map.get(regime_label, px.colors.qualitative.Set2[i])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=regime_data['Date'], \n",
    "                y=regime_data['Close'],\n",
    "                mode='markers',\n",
    "                marker=dict(color=color, size=6),\n",
    "                name=f'{regime_label} Regime',\n",
    "                hovertemplate='%{x}<br>Price: %{y:.2f}<br>Regime: ' + regime_label + \n",
    "                              '<br>Probability: %{text:.2f}',\n",
    "                text=regime_data['Regime_Probability']\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Add regime probability traces in the second subplot\n",
    "    regime_labels = [col.replace('Prob_', '') for col in results.columns if col.startswith('Prob_')]\n",
    "    for label in regime_labels:\n",
    "        color = color_map.get(label, 'gray')\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=results['Date'],\n",
    "                y=results[f'Prob_{label}'],\n",
    "                mode='lines',\n",
    "                line=dict(width=2, color=color),\n",
    "                name=f'{label} Probability'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Add a horizontal line at 0.5 probability\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=results['Date'].min(),\n",
    "        y0=0.5,\n",
    "        x1=results['Date'].max(),\n",
    "        y1=0.5,\n",
    "        line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title=f'{TICKER} Price',\n",
    "        yaxis2_title='Probability',\n",
    "        template='plotly_white',\n",
    "        legend_title='Market Regimes',\n",
    "        hovermode='closest',\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    # Set y-axis range for probability subplot\n",
    "    fig.update_yaxes(range=[0, 1], row=2, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create pie chart showing regime distribution\n",
    "    regime_distribution = results['Regime_Label'].value_counts().reset_index()\n",
    "    regime_distribution.columns = ['Regime', 'Days']\n",
    "    regime_distribution['Percentage'] = regime_distribution['Days'] / len(results) * 100\n",
    "    \n",
    "    # Use the same colors for pie chart\n",
    "    pie_colors = [color_map.get(label, 'gray') for label in regime_distribution['Regime']]\n",
    "    \n",
    "    fig_pie = px.pie(\n",
    "        regime_distribution, \n",
    "        values='Percentage', \n",
    "        names='Regime',\n",
    "        title=f'Regime Distribution ({results[\"Date\"].min().strftime(\"%Y-%m-%d\")} to {results[\"Date\"].max().strftime(\"%Y-%m-%d\")})',\n",
    "        color='Regime',\n",
    "        color_discrete_map={label: color_map.get(label, 'gray') for label in regime_distribution['Regime']}\n",
    "    )\n",
    "    \n",
    "    fig_pie.update_traces(textinfo='percent+label', textposition='inside')\n",
    "    fig_pie.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\n",
    "    \n",
    "    fig_pie.show()\n",
    "\n",
    "def plot_feature_importance(model_bundle, training_data):\n",
    "    \"\"\"\n",
    "    Plot feature importance for the GMM model based on component means and weights\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        model = model_bundle['model']\n",
    "        features = model_bundle['features']\n",
    "        regime_labels = model_bundle['regime_labels']\n",
    "        \n",
    "        # Get means and weights from the model\n",
    "        means = model.means_\n",
    "        weights = model.weights_\n",
    "        \n",
    "        # Plot means for each feature across components\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Create heatmap of means\n",
    "        sns.heatmap(means, cmap='RdBu_r', center=0, \n",
    "                    xticklabels=features, \n",
    "                    yticklabels=[f\"Regime {i} ({regime_labels[i]})\" for i in range(len(regime_labels))],\n",
    "                    annot=True, fmt=\".2f\")\n",
    "        \n",
    "        plt.title('Feature Importance by Regime (Standardized Feature Means)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot PCA for feature visualization\n",
    "        feature_data = training_data[features].values\n",
    "        scaler = model_bundle['scaler']\n",
    "        scaled_features = scaler.transform(feature_data)\n",
    "        \n",
    "        # Reduce to 2D using PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_features = pca.fit_transform(scaled_features)\n",
    "        \n",
    "        # Get component contributions\n",
    "        component_loadings = pca.components_.T\n",
    "        variance_ratio = pca.explained_variance_ratio_\n",
    "        \n",
    "        # Create scatter plot of data points colored by regime\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        predictions = model.predict(scaled_features)\n",
    "        \n",
    "        # Create a DataFrame with reduced features and cluster labels\n",
    "        viz_df = pd.DataFrame({\n",
    "            'PC1': reduced_features[:, 0],\n",
    "            'PC2': reduced_features[:, 1],\n",
    "            'Regime': predictions,\n",
    "            'Label': [regime_labels[p] for p in predictions]\n",
    "        })\n",
    "        \n",
    "        # Custom colors for regimes\n",
    "        color_map = {'Bull': 'green', 'Bear': 'red', 'Neutral': 'gold'}\n",
    "        \n",
    "        # Plot data points\n",
    "        for label, group in viz_df.groupby('Label'):\n",
    "            plt.scatter(group['PC1'], group['PC2'], \n",
    "                       label=label, \n",
    "                       color=color_map.get(label, 'gray'), \n",
    "                       alpha=0.5)\n",
    "        \n",
    "        # Plot centroids\n",
    "        centroids_pca = pca.transform(means)\n",
    "        for i, (x, y) in enumerate(centroids_pca):\n",
    "            plt.scatter(x, y, s=300, marker='*', \n",
    "                       color=color_map.get(regime_labels[i], 'gray'), \n",
    "                       edgecolor='k', linewidth=2)\n",
    "            plt.annotate(regime_labels[i], (x, y), xytext=(5, 5), \n",
    "                        textcoords='offset points', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Plot feature vectors\n",
    "        for i, feature in enumerate(features):\n",
    "            plt.arrow(0, 0, component_loadings[i, 0]*max(abs(reduced_features[:, 0]))*0.7, \n",
    "                     component_loadings[i, 1]*max(abs(reduced_features[:, 1]))*0.7, \n",
    "                     head_width=0.05, head_length=0.05, fc='blue', ec='blue')\n",
    "            plt.text(component_loadings[i, 0]*max(abs(reduced_features[:, 0]))*0.8, \n",
    "                    component_loadings[i, 1]*max(abs(reduced_features[:, 1]))*0.8, \n",
    "                    feature, color='blue', ha='center', va='center')\n",
    "        \n",
    "        plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "        plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "        plt.title('GMM Regimes with Feature Vectors in PCA Space')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Visualization requires matplotlib and seaborn. Install with: pip install matplotlib seaborn\")\n",
    "\n",
    "# %%\n",
    "# Additional analysis tools for GMM-based regime detection\n",
    "\n",
    "def apply_probability_smoothing(probabilities, window_size=5):\n",
    "    \"\"\"\n",
    "    Apply smoothing to regime probabilities rather than discrete labels\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    probabilities : array-like of shape (n_samples, n_components)\n",
    "        Probabilities for each sample belonging to each regime\n",
    "    window_size : int\n",
    "        Size of the rolling window for smoothing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    array-like of shape (n_samples, n_components)\n",
    "        Smoothed probabilities\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convert to DataFrame for easy rolling operations\n",
    "    prob_df = pd.DataFrame(probabilities)\n",
    "    \n",
    "    # Apply rolling mean to each probability column\n",
    "    smoothed_probs = prob_df.rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    # Fill NaN values at edges with original probabilities\n",
    "    for col in prob_df.columns:\n",
    "        smoothed_probs[col] = smoothed_probs[col].fillna(prob_df[col])\n",
    "    \n",
    "    # Renormalize so probabilities sum to 1\n",
    "    row_sums = smoothed_probs.sum(axis=1)\n",
    "    for col in smoothed_probs.columns:\n",
    "        smoothed_probs[col] = smoothed_probs[col] / row_sums\n",
    "    \n",
    "    return smoothed_probs.values\n",
    "\n",
    "def analyze_with_smoothing(model_bundle, data, start_date, end_date, window_size=5, title=None):\n",
    "    \"\"\"\n",
    "    Analyze a time period with smoothed probabilities to reduce regime-switching noise\n",
    "    \"\"\"\n",
    "    # Extract model components\n",
    "    model = model_bundle['model']\n",
    "    scaler = model_bundle['scaler']\n",
    "    features = model_bundle['features']\n",
    "    regime_labels = model_bundle['regime_labels']\n",
    "    \n",
    "    # Filter data for prediction period\n",
    "    pred_data = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)].copy()\n",
    "    \n",
    "    if len(pred_data) == 0:\n",
    "        print(f\"No data available for period {start_date} to {end_date}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    feature_data = pred_data[features].values\n",
    "    \n",
    "    # Scale features using the same scaler as training\n",
    "    scaled_features = scaler.transform(feature_data)\n",
    "    \n",
    "    # Get raw probabilities\n",
    "    raw_probabilities = model.predict_proba(scaled_features)\n",
    "    \n",
    "    # Apply smoothing to the probabilities\n",
    "    smoothed_probabilities = apply_probability_smoothing(raw_probabilities, window_size)\n",
    "    \n",
    "    # Get predictions from smoothed probabilities\n",
    "    predictions = np.argmax(smoothed_probabilities, axis=1)\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    pred_data['Predicted_Regime'] = predictions\n",
    "    pred_data['Regime_Label'] = [regime_labels[r] for r in predictions]\n",
    "    pred_data['Regime_Probability'] = np.max(smoothed_probabilities, axis=1)\n",
    "    \n",
    "    # Add probability columns\n",
    "    for i, label in enumerate(regime_labels):\n",
    "        pred_data[f'Prob_{label}'] = smoothed_probabilities[:, i]\n",
    "    \n",
    "    # For comparison, also compute raw predictions\n",
    "    raw_predictions = model.predict(scaled_features)\n",
    "    pred_data['Raw_Regime'] = raw_predictions\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(f\"\\nSmoothed Regime Prediction (window={window_size}) from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Calculate regime distribution\n",
    "    regime_counts = pd.Series(predictions).value_counts(normalize=True) * 100\n",
    "    print(\"\\nRegime Distribution (smoothed):\")\n",
    "    for regime, percentage in sorted(regime_counts.items()):\n",
    "        print(f\"Regime {regime} [{regime_labels[regime]}]: {percentage:.2f}%\")\n",
    "    \n",
    "    # Calculate average regime duration\n",
    "    regime_changes = (pred_data['Predicted_Regime'] != pred_data['Predicted_Regime'].shift(1)).sum()\n",
    "    avg_duration = len(pred_data) / (regime_changes if regime_changes > 0 else 1)\n",
    "    print(f\"\\nRegime persistence (smoothed): {avg_duration:.2f} days average duration\")\n",
    "    \n",
    "    # Compare with raw predictions\n",
    "    raw_changes = (pred_data['Raw_Regime'] != pred_data['Raw_Regime'].shift(1)).sum()\n",
    "    raw_duration = len(pred_data) / (raw_changes if raw_changes > 0 else 1)\n",
    "    print(f\"Regime persistence (raw): {raw_duration:.2f} days average duration\")\n",
    "    print(f\"Smoothing reduced regime changes by {(raw_changes - regime_changes) / raw_changes * 100:.1f}%\")\n",
    "    \n",
    "    # Create a custom title if not provided\n",
    "    if title is None:\n",
    "        title = f'Smoothed Market Regimes (window={window_size}) from {start_date} to {end_date}'\n",
    "    \n",
    "    # Plot the results\n",
    "    plot_regimes(pred_data, title)\n",
    "    \n",
    "    return pred_data\n",
    "\n",
    "# ======== MAIN EXECUTION ========\n",
    "def main():\n",
    "    # Download and prepare data\n",
    "    df = download_market_data(TICKER, VIX_TICKER, TNX_TICKER, START_DATE)\n",
    "    df = calculate_features(df)\n",
    "    \n",
    "    # Train model\n",
    "    model_bundle, training_data, train_predictions, regime_labels, regime_stats = train_gmm_model(\n",
    "        df, TRAIN_START_DATE, TRAIN_END_DATE, NUM_REGIMES, MAX_ITERATIONS, N_INIT, COVARIANCE_TYPE\n",
    "    )\n",
    "    \n",
    "    # Store regime labels in the model bundle for convenience\n",
    "    model_bundle['regime_labels'] = regime_labels\n",
    "    \n",
    "    # Plot training period results\n",
    "    training_results = training_data.copy()\n",
    "    training_results['Predicted_Regime'] = train_predictions\n",
    "    training_results['Regime_Label'] = [regime_labels[r] for r in train_predictions]\n",
    "    \n",
    "    # Add probability columns to training results\n",
    "    probabilities = model_bundle['model'].predict_proba(\n",
    "        model_bundle['scaler'].transform(training_data[model_bundle['features']].values)\n",
    "    )\n",
    "    training_results['Regime_Probability'] = np.max(probabilities, axis=1)\n",
    "    for i, label in enumerate(regime_labels):\n",
    "        training_results[f'Prob_{label}'] = probabilities[:, i]\n",
    "    \n",
    "    print(\"\\nVisualization of training period regimes:\")\n",
    "    plot_regimes(training_results, f'Market Regimes - Training Period ({TRAIN_START_DATE} to {TRAIN_END_DATE})')\n",
    "    \n",
    "    # Plot feature importance (PCA space visualization)\n",
    "    try:\n",
    "        plot_feature_importance(model_bundle, training_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot feature importance: {e}\")\n",
    "    \n",
    "    # Example: Analyze the most recent period\n",
    "    today = datetime.today()\n",
    "    recent_start = '2024-01-02'\n",
    "    recent_end = (datetime.today() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    print(f\"\\nPredicting regimes for recent period ({recent_start} to {recent_end}):\")\n",
    "    recent_results = predict_regimes(model_bundle, df, recent_start, recent_end)\n",
    "    plot_regimes(recent_results, f'Market Regimes - Recent Period ({recent_start} to {recent_end})')\n",
    "    \n",
    "    # Analyze recent period with probability smoothing\n",
    "    print(f\"\\nAnalyzing recent period with probability smoothing (2024-01-01 to 2025-04-14):\")\n",
    "    smoothed_results = analyze_with_smoothing(model_bundle, df, recent_start,recent_end, window_size=14)\n",
    "    \n",
    "    return {\n",
    "        'model_bundle': model_bundle,\n",
    "        'data': df,\n",
    "        'training_results': training_results,\n",
    "        'smoothed_results': smoothed_results  # Now we return the smoothed results\n",
    "    }\n",
    "\n",
    "# Function to analyze any time period\n",
    "def analyze_period(model_objects, start_date, end_date, title=None):\n",
    "    \"\"\"Analyze any time period using the trained model\"\"\"\n",
    "    results = predict_regimes(\n",
    "        model_objects['model_bundle'], \n",
    "        model_objects['data'], \n",
    "        start_date, \n",
    "        end_date\n",
    "    )\n",
    "    \n",
    "    if results is not None:\n",
    "        plot_regimes(results, title)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    model_objects = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy Review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing strategy with smoothed regime classifications...\n",
      "Strategy period: 2024-01-02 00:00:00 to 2025-04-16 00:00:00\n",
      "Number of trading days: 324\n",
      "Downloading historical prices for ['SPY', 'GLD', 'SPUU']...\n",
      "Strategy final value: $14355.47\n",
      "Buy & Hold final value: $11651.92\n",
      "Generating QuantStats tearsheet...\n",
      "Error generating tearsheet: pivot() takes 1 positional argument but 4 were given\n",
      "Falling back to basic metrics...\n",
      "\n",
      "Strategy Performance Metrics:\n",
      "Cumulative Return: 43.55%\n",
      "Annualized Return: 32.57%\n",
      "Sharpe Ratio: 2.23\n",
      "Max Drawdown: -5.54%\n",
      "Win Rate: 57.45%\n",
      "\n",
      "Benchmark Performance Metrics:\n",
      "Cumulative Return: 15.55%\n",
      "Annualized Return: 11.93%\n",
      "Sharpe Ratio: 0.72\n",
      "Max Drawdown: -18.76%\n",
      "Win Rate: 57.45%\n"
     ]
    }
   ],
   "source": [
    "# Strategy Implementation and Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import quantstats as qs\n",
    "from datetime import datetime\n",
    "\n",
    "def implement_regime_strategy(smoothed_results, test_start_date, test_end_date):\n",
    "    \"\"\"\n",
    "    Implement a regime-based trading strategy with proper handling of look-ahead bias.\n",
    "    \n",
    "    Strategy allocations by regime:\n",
    "    - Bull Regime: 50% SPY, 50% Leveraged SPY (SPUU)\n",
    "    - Bear Regime: 50% Cash, 50% GLD    \n",
    "    - Neutral Regime: 100% SPY\n",
    "    \"\"\"\n",
    "    # 1. Extract regime labels from smoothed predictions\n",
    "    regime_data = smoothed_results[['Date', 'Regime_Label']].copy()\n",
    "    print(f\"Strategy period: {test_start_date} to {test_end_date}\")\n",
    "    print(f\"Number of trading days: {len(regime_data)}\")\n",
    "    \n",
    "    # 2. Download price data for required assets\n",
    "    tickers = ['SPY', 'GLD', 'SPUU']\n",
    "    print(f\"Downloading historical prices for {tickers}...\")\n",
    "    asset_prices = yf.download(tickers, start=test_start_date, end=test_end_date, progress=False)\n",
    "    \n",
    "    # 3. Prepare the portfolio tracking dataframe\n",
    "    portfolio = pd.DataFrame(index=asset_prices.index)\n",
    "    portfolio['Regime'] = None\n",
    "    portfolio['SPY_Weight'] = 0.0\n",
    "    portfolio['GLD_Weight'] = 0.0\n",
    "    portfolio['SPUU_Weight'] = 0.0\n",
    "    portfolio['Cash_Weight'] = 0.0\n",
    "    \n",
    "    # Align dates between regime data and portfolio\n",
    "    for date in portfolio.index:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        matching_rows = regime_data[regime_data['Date'] == date_str]\n",
    "        if not matching_rows.empty:\n",
    "            portfolio.loc[date, 'Regime'] = matching_rows.iloc[0]['Regime_Label']\n",
    "    \n",
    "    # Fill any missing regimes with forward fill (maintain previous regime)\n",
    "    portfolio['Regime'] = portfolio['Regime'].fillna(method='ffill')\n",
    "    \n",
    "    # 4. Set initial portfolio value and track daily values\n",
    "    initial_portfolio_value = 10000  # $10,000 starting capital\n",
    "    portfolio['Portfolio_Value'] = initial_portfolio_value\n",
    "    \n",
    "    # Track positions and values\n",
    "    portfolio['SPY_Units'] = 0.0\n",
    "    portfolio['GLD_Units'] = 0.0\n",
    "    portfolio['SPUU_Units'] = 0.0\n",
    "    portfolio['Cash_Value'] = initial_portfolio_value\n",
    "    \n",
    "    # 5. Implement strategy with 1-day lag to avoid look-ahead bias\n",
    "    current_regime = None\n",
    "    \n",
    "    # Loop through each day (starting from second day to implement lag)\n",
    "    for i in range(1, len(portfolio)):\n",
    "        prev_date = portfolio.index[i-1]\n",
    "        curr_date = portfolio.index[i]\n",
    "        \n",
    "        # Get regime from previous day's close\n",
    "        prev_regime = portfolio.loc[prev_date, 'Regime']\n",
    "        \n",
    "        # Check if regime has changed\n",
    "        if prev_regime != current_regime:\n",
    "            current_regime = prev_regime\n",
    "            \n",
    "            # Calculate new target weights based on regime\n",
    "            if current_regime == 'Bull':\n",
    "                portfolio.loc[curr_date, 'SPY_Weight'] = 0.5\n",
    "                portfolio.loc[curr_date, 'SPUU_Weight'] = 0.5\n",
    "                portfolio.loc[curr_date, 'GLD_Weight'] = 0.0\n",
    "                portfolio.loc[curr_date, 'Cash_Weight'] = 0.0\n",
    "            elif current_regime == 'Bear':\n",
    "                portfolio.loc[curr_date, 'SPY_Weight'] = 0.0\n",
    "                portfolio.loc[curr_date, 'SPUU_Weight'] = 0.0\n",
    "                portfolio.loc[curr_date, 'GLD_Weight'] = 0.5\n",
    "                portfolio.loc[curr_date, 'Cash_Weight'] = 0.5\n",
    "            else:  # Neutral\n",
    "                portfolio.loc[curr_date, 'SPY_Weight'] = 1.0\n",
    "                portfolio.loc[curr_date, 'SPUU_Weight'] = 0.0\n",
    "                portfolio.loc[curr_date, 'GLD_Weight'] = 0.0\n",
    "                portfolio.loc[curr_date, 'Cash_Weight'] = 0.0\n",
    "            \n",
    "            # Get yesterday's portfolio value\n",
    "            prev_portfolio_value = portfolio.loc[prev_date, 'Portfolio_Value']\n",
    "            \n",
    "            # Calculate new positions based on today's open prices\n",
    "            spy_open = asset_prices['Open']['SPY'][curr_date]\n",
    "            gld_open = asset_prices['Open']['GLD'][curr_date]\n",
    "            spuu_open = asset_prices['Open']['SPUU'][curr_date]\n",
    "            \n",
    "            # Calculate units to hold for each asset\n",
    "            portfolio.loc[curr_date, 'SPY_Units'] = (portfolio.loc[curr_date, 'SPY_Weight'] * prev_portfolio_value) / spy_open\n",
    "            portfolio.loc[curr_date, 'GLD_Units'] = (portfolio.loc[curr_date, 'GLD_Weight'] * prev_portfolio_value) / gld_open\n",
    "            portfolio.loc[curr_date, 'SPUU_Units'] = (portfolio.loc[curr_date, 'SPUU_Weight'] * prev_portfolio_value) / spuu_open\n",
    "            portfolio.loc[curr_date, 'Cash_Value'] = portfolio.loc[curr_date, 'Cash_Weight'] * prev_portfolio_value\n",
    "        else:\n",
    "            # No regime change, maintain same positions\n",
    "            portfolio.loc[curr_date, 'SPY_Weight'] = portfolio.loc[prev_date, 'SPY_Weight']\n",
    "            portfolio.loc[curr_date, 'GLD_Weight'] = portfolio.loc[prev_date, 'GLD_Weight']\n",
    "            portfolio.loc[curr_date, 'SPUU_Weight'] = portfolio.loc[prev_date, 'SPUU_Weight']\n",
    "            portfolio.loc[curr_date, 'Cash_Weight'] = portfolio.loc[prev_date, 'Cash_Weight']\n",
    "            \n",
    "            portfolio.loc[curr_date, 'SPY_Units'] = portfolio.loc[prev_date, 'SPY_Units']\n",
    "            portfolio.loc[curr_date, 'GLD_Units'] = portfolio.loc[prev_date, 'GLD_Units']\n",
    "            portfolio.loc[curr_date, 'SPUU_Units'] = portfolio.loc[prev_date, 'SPUU_Units']\n",
    "            portfolio.loc[curr_date, 'Cash_Value'] = portfolio.loc[prev_date, 'Cash_Value']\n",
    "        \n",
    "        # Calculate portfolio value at the end of the current day\n",
    "        spy_close = asset_prices['Close']['SPY'][curr_date]\n",
    "        gld_close = asset_prices['Close']['GLD'][curr_date]\n",
    "        spuu_close = asset_prices['Close']['SPUU'][curr_date]\n",
    "        \n",
    "        spy_value = portfolio.loc[curr_date, 'SPY_Units'] * spy_close\n",
    "        gld_value = portfolio.loc[curr_date, 'GLD_Units'] * gld_close\n",
    "        spuu_value = portfolio.loc[curr_date, 'SPUU_Units'] * spuu_close\n",
    "        cash_value = portfolio.loc[curr_date, 'Cash_Value']\n",
    "        \n",
    "        portfolio.loc[curr_date, 'Portfolio_Value'] = spy_value + gld_value + spuu_value + cash_value\n",
    "    \n",
    "    # 6. Calculate daily returns\n",
    "    portfolio['Daily_Return'] = portfolio['Portfolio_Value'].pct_change()\n",
    "    \n",
    "    # 7. Create SPY benchmark returns\n",
    "    spy_prices = asset_prices['Close']['SPY']\n",
    "    spy_returns = spy_prices.pct_change()\n",
    "    \n",
    "    # 8. Prepare returns for QuantStats - ensure proper Series format\n",
    "    strategy_returns = portfolio['Daily_Return'].dropna()\n",
    "    benchmark_returns = spy_returns.loc[strategy_returns.index]\n",
    "    \n",
    "    print(f\"Strategy final value: ${portfolio['Portfolio_Value'].iloc[-1]:.2f}\")\n",
    "    print(f\"Buy & Hold final value: ${initial_portfolio_value * (1 + benchmark_returns.sum()):.2f}\")\n",
    "    \n",
    "    return strategy_returns, benchmark_returns, portfolio\n",
    "\n",
    "# Generate the tearsheet using QuantStats\n",
    "def generate_tearsheet(strategy_returns, benchmark_returns):\n",
    "    \"\"\"Generate a QuantStats tearsheet comparing strategy to SPY benchmark\"\"\"\n",
    "    print(\"Generating QuantStats tearsheet...\")\n",
    "    \n",
    "    # Ensure returns are properly formatted for QuantStats\n",
    "    # Make sure they are pandas Series with datetime index\n",
    "    if not isinstance(strategy_returns, pd.Series):\n",
    "        strategy_returns = pd.Series(strategy_returns)\n",
    "    \n",
    "    if not isinstance(benchmark_returns, pd.Series):\n",
    "        benchmark_returns = pd.Series(benchmark_returns)\n",
    "    \n",
    "    # Ensure index is DatetimeIndex\n",
    "    if not isinstance(strategy_returns.index, pd.DatetimeIndex):\n",
    "        strategy_returns.index = pd.DatetimeIndex(strategy_returns.index)\n",
    "    \n",
    "    if not isinstance(benchmark_returns.index, pd.DatetimeIndex):\n",
    "        benchmark_returns.index = pd.DatetimeIndex(benchmark_returns.index)\n",
    "    \n",
    "    # Set names for clarity\n",
    "    strategy_returns.name = 'Strategy'\n",
    "    benchmark_returns.name = 'SPY'\n",
    "    \n",
    "    try:\n",
    "        # Generate the tearsheet\n",
    "        qs.reports.html(strategy_returns, benchmark_returns, \n",
    "                        title='GMM Regime Strategy vs. S&P 500', \n",
    "                        output='regime_strategy_tearsheet.html')\n",
    "        print(\"Tearsheet saved as 'regime_strategy_tearsheet.html'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating tearsheet: {e}\")\n",
    "        print(\"Falling back to basic metrics...\")\n",
    "        \n",
    "        # If tearsheet fails, at least show some key metrics\n",
    "        print(\"\\nStrategy Performance Metrics:\")\n",
    "        print(f\"Cumulative Return: {(strategy_returns + 1).prod() - 1:.2%}\")\n",
    "        print(f\"Annualized Return: {qs.stats.cagr(strategy_returns):.2%}\")\n",
    "        print(f\"Sharpe Ratio: {qs.stats.sharpe(strategy_returns):.2f}\")\n",
    "        print(f\"Max Drawdown: {qs.stats.max_drawdown(strategy_returns):.2%}\")\n",
    "        print(f\"Win Rate: {qs.stats.win_rate(strategy_returns):.2%}\")\n",
    "        \n",
    "        print(\"\\nBenchmark Performance Metrics:\")\n",
    "        print(f\"Cumulative Return: {(benchmark_returns + 1).prod() - 1:.2%}\")\n",
    "        print(f\"Annualized Return: {qs.stats.cagr(benchmark_returns):.2%}\")\n",
    "        print(f\"Sharpe Ratio: {qs.stats.sharpe(benchmark_returns):.2f}\")\n",
    "        print(f\"Max Drawdown: {qs.stats.max_drawdown(benchmark_returns):.2%}\")\n",
    "        print(f\"Win Rate: {qs.stats.win_rate(benchmark_returns):.2%}\")\n",
    "\n",
    "# Execute strategy and evaluate\n",
    "def run_strategy_evaluation(smoothed_results):\n",
    "    \"\"\"Run the full strategy implementation and evaluation\"\"\"\n",
    "    # Get the date range from the smoothed results\n",
    "    test_start_date = smoothed_results['Date'].min()\n",
    "    test_end_date = smoothed_results['Date'].max()\n",
    "    \n",
    "    # Implement strategy\n",
    "    strategy_returns, benchmark_returns, portfolio = implement_regime_strategy(\n",
    "        smoothed_results, test_start_date, test_end_date\n",
    "    )\n",
    "    \n",
    "    # Generate tearsheet for evaluation\n",
    "    generate_tearsheet(strategy_returns, benchmark_returns)\n",
    "    \n",
    "    return {\n",
    "        'strategy_returns': strategy_returns,\n",
    "        'benchmark_returns': benchmark_returns,\n",
    "        'portfolio': portfolio\n",
    "    }\n",
    "\n",
    "# Example usage - access smoothed_results from model_objects\n",
    "# Run the main function first to get all required data\n",
    "if 'model_objects' not in globals():\n",
    "    print(\"Running main model to generate regime classifications...\")\n",
    "    model_objects = main()\n",
    "\n",
    "# Access the smoothed results from model_objects\n",
    "if model_objects and 'smoothed_results' in model_objects:\n",
    "    print(\"Executing strategy with smoothed regime classifications...\")\n",
    "    strategy_results = run_strategy_evaluation(model_objects['smoothed_results'])\n",
    "else:\n",
    "    print(\"No smoothed results available. Run the GMM model first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
